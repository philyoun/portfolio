---
title: "ESL4.3 Linear Discriminant Analysis"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
library(tidyverse)
```
 Section 2.4에서 decision theory for classification에 대해 다뤘던 적이 있다. 
 
 optimal classification이라는 건 결국 class posteriors $Pr(G=k|X=x)$가 최대가 되는 값으로 $x$를 분류(classify)한다는 것. 
 
 주어진 $x$에 대해, $k = 1, 2, ..., K$라는 그룹에 따라서 확률이 다 있을텐데, 그 중에서 가장 이 값이 큰 그룹으로 분류를 하겠다는 것. 
 
 이걸 Bayes' thm에 따라서 써보면, $Pr(G=k|X=x) = \frac{f_k(x)}{\sum_{l = 1}^{K}f_l(x)\pi_l}$  이 된다. 
 
 여기서 $f_k(x)$ 는 주어진 그룹에 따라 $x$의 확률이 얼마나 되느냐? 하는 class-conditional density of $X$ in class $G = k$인 likelihood의 역할을 한다. 
 
 $\pi_k$는 prior probability of class k, 즉, 그룹 k일 확률이 얼마나 되느냐? 하는 것. 예를 들어서, R의 기본 데이터인 iris를 불러보면, 이 데이터는 Species가 3 종류이고 각각 50개씩 총 150개의 데이터가 있다. 이 경우에는 prior가 단순하게 $\frac{1}{3}$이 된다. 
 
```{r echo=TRUE}
iris %>% 
  group_by(Species) %>% 
  count
```
 

 많은 테크닉들은 class densities 모델들에 기반해 있는데(based on), 그러니깐 데이터가 어떠한 분포를 하고 있을거다, 가정을 하고 진행하는데, 
 
 1. LDA(Linear Discriminant Analysis), QDA(Quadratic Discriminant Analysis)는 Gaussian 분포를 이용하고,
 2. 좀 더 flexible한 mixture of Gaussian 분포는 nonlinear decision boundaries를 가능하게 한다.
 3. 각 class에 대해 general nonparametric density estimates를 하면 most flexibility를 얻을 수 있을 것이다.
 4. Naive Bayes 모델은 3번 케이스의 변형인데, 각 class densities가 marginal densities의 products라는 걸 가정한다.
 즉, inputs가 각 class별로 conditionally independent하다라는 걸 의미.(Section 6.6.3에서 더 자세하게 다룸)
 
 
### LDA
 자, 여기서 우리는 1번 케이스에 대해서 다루고 있는 것이니깐, Gaussian 분포를 사용할 것임.
 
 class density인 $f_k(x)$를 다음과 같이 두자. $f_k(x) = \frac{1}{(2\pi)^{p/2}{\left |\Sigma_k  \right |}^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}$ 이라는 multivariate Gaussian
 
 이 중에서도 LDA는 $\Sigma_k = \Sigma$라는 스페셜한 케이스를 가정할 때 성립하는 것임. $X=x$일 때, 두 classes k와 l 중에 어디에 포함될까? 하는 걸 다음과 같이 써볼 수 있다. $\frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} > 1$ 이라면 class k에 속할 확률이 class l에 속할 확률보다 크다는 것이니깐 k class로 분류하는 것이고, vice versa. 만약에 $=1$이라면? 그럼 class k로 분류하든, class l로 분류하든 상관이 없다. 같은 확률이라는 뜻이니깐. 그리고 이 때가 decision boundary가 되는 것이다. 
 
 보통 저 분수값 앞에다가 log를 붙여서, log-ratio를 사용하는데, 이유는 Gaussian에 $e$가 붙어있기 때문. log-ratio로 써보면, $log\frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} = log\frac{f_k(x)}{f_l(x)} + log\frac{\pi_k}{\pi_l}\cdots (4.9)$ 가 되고, 이걸 $x$에 대해 정리해보면, $log\frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T\Sigma^{-1}(\mu_k - \mu_l)+x^T\Sigma^{-1}(\mu_k-\mu_l)$로써, 마지막 항에서 볼 수 있듯이 **equation linear in x**라는 걸 알 수 있다. $\Sigma_k$라는 specific matrix를 사용하는 것 대신, common covariance matrix $\Sigma$를 사용하기로 했기 때문에, 약분이 잘 되어서 저렇게 깔끔하게 나오는 것을 확인할 수 있다. 
 
 (반대로 common covariance matrix $\Sigma$ 대신에 specific matrix $\Sigma_k$를 쓰기로 한다면 위의 log-ratio가 약분이 안 되고 **x에 대한 quadratic form**으로 나오기 때문에, 이걸 Quadratic Discriminant Analysis, QDA라고 부른다.)
 
 이렇게 linear log-odds function이란건 classes k와 l간에 decision boundary, 그러니깐 $\frac{Pr(G = k|X = x)}{Pr(G = l|X = x)}$인 $x$를 모아놓은 set이 linear하다는 걸 뜻한다. 즉, x변수가 p개라면, p차원에서의 hyperplane이 linear decision boundary가 되는 것.  
 
 그리고 물론 어떠한 2개의 classes에 대해서도 이게 성립하니깐 모든 decision boundaries는 linear하다는 걸 알 수 있다. 
![](4.3_LDA_files\figure-html\fig4.11.png)
 
 위 사진은 앞으로 보게 될 데이터에 대해 LDA에서 얻은 linear decision boundaries를 그어놓은 것인데, 다 Linear한 것을 볼 수 있다.
 
 식 (4.9)를 좀 더 자세하게 봐보자. 이 경우에는 2가지 classes에 대해서만 log-ratio를 비교해본 것인데, classes가 클 때에 있어서 이렇게 2개씩 일일이 비교할 수는 없다. 그보단 이렇게, $\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+log\pi_k \cdots (4.10)$로 각각의 class에 대해 $\delta_k(x)$값을 구하고, 가장 큰 값이 되는 class로 분류한다. 이걸 linear discriminant function이라고 부른다. 그럼 decision rule은? $G(x) = argmax_k\delta_k(x)$가 되는 것. 이게 식 (4.9)와 같다는 건, $\delta_k(x) > \delta_l(x)$를 풀어서 써보면 알 수 있다.
 
 practice에서는 Gaussian 분포에 대한 모수들을 알지 못한다. 그렇기 때문에 estimate를 training data에서 구해서 써야 된다. 
 
 - $\hat{\pi}_k = N_k/N$, $N$은 전체 observation 수, $N_k$는 해당 클래스 k observation 수
 - $\hat{\mu}_k = \sum_{g_i = k}x_i/N_k$, 즉, 해당 클래스 k의 값들의 합 / 해당 클래스 k observation 수
 - $\hat{\Sigma} = \sum_{k = 1}^{K}\sum_{g_i = k}(x_i - \hat\mu_k)(x_i - \hat\mu_k)^T/(N-K)$, 이렇게 common covariance matrix를 구한다. 


 2개의 classes만 있는 경우에는, LDA와 linear regression으로 선을 그어서 분류하는 것과 연관이 있다. 
 
 결론부터 말하면, 식 (4.9) 다음 식의 $x^T\Sigma^{-1}(\mu_k-\mu_l)$이라는 x의 계수가, class 1일 때에는 1로 두고, class 2일 때는 -1로 둔 다음 회귀계수를 구했을 때의 x의 계수와 비례한다. 교재에서는 LDA direction과 coefficient vector from least squares가 proportional. 당연히 1, -1이 아니라 distinct한 값으로 둬도 비례함. 하지만 $N_1 = N_2$가 아닌 경우에는 intercepts 값이 달라지면서 resulting decision rule도 달라진다. 증명...을 추가해야할텐데
 
 자, 이렇게 분류를 하는 Linear decision boundary를 구하는데 있어, 2가지 방법으로 할 수 있다는 걸 봤다. 처음에는 Gaussian 분포를 가정한 LDA방법으로 했지만, 2번째로 Linear regression으로 할 때에도 proportional한 coefficient vector를 구할 수 있다는 걸 봤다. 이러면 꼭 데이터가 Gaussian 분포를 하지 않아도 LDA를 쓸 수 있다는 것이다! 어떠한 데이터에 대해서 해도 상관이 업서짐. 그런데 intercept 부분을 구하는데 있어서는 또다시 Gaussian 분포 가정을 했다. 그래서, training error를 줄이는 cut-point를 시행착오로(empirically) 최소화하겠다. 이것이 우리가 실제 데이터를 하면서 얻은 결론이다. 그런데 아무데에도 써져 있지는 않더라. 라는 얘기를 한다.
 

### QDA
 이번엔 더 이상 $\Sigma_k = \Sigma$가 아닌 경우를 다뤄보자. 이러면 식 (4.9)에서 $\Sigma_k$가 더 이상 약분되지 않는다. x에 대해 quadratic한 form이 계속 남아있게 됨. 그래서 LDA 때의 linear discriminant function과 마찬가지로 quadratic discriminant function을 써보면, $\delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) + log\pi_k \cdots (4.12)$. 그리고 이 경우에 decision boundary는 x의 quadratic equation이 된다.$\{x|\delta_k(x) = \delta_l(x)\}$인 점들의 집합.
 
 꼭 이렇게 QDA로 quadratic boundaries를 찾을 필요는 없고, 다른 방법으로 decision boundaries를 approx.하는 방법도 있는데, 자주 쓰이는 방법으로 enlarged quadratic polynomial space에다가 LDA를 적용. 기존에 $X_1, X_2$가 있다면 이걸 $X_1, X_2, X_1X_2, X_1^2, X_2^2$f로 space를 확장시키는 것임. 회귀분석을 할 때 변수들 늘리는 것 생각하면 될 듯.
 
 QDA가 더 선호되는 접근이고, 위 방법이 편리한 대체재.
 
 QDA를 할 때의 문제는, 각 class에 대해 다 따로 covariance matrices를 구해주어야 한다는 것. 그래서 변수가 많아지면 추정해야할 parameters가 기하급수적으로 많아지는게 문제. 


```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
