---
# $\SS4.3 Linear Discriminant Analysis, LDA
---
Section 2.4에서 했던 것처럼, classification을 위한 decision theory에서 말하는 건 그거다.

Optimal classification을 위해서는 class posteriors $Pr(G|X)$를 알아야 한다는 것. 
이 $Pr(G|X)$라는 건, $Pr(G = k | X = x)$로, $x$가 주어졌을 때, 어떠한 class $G = k$ 로 classify할 것이냐라는 뜻이다. 

class $k$에 대한 prior가 $\pi_k$ 로 주어져 있고, $f_k(x)$ 가 class k에 대한 class-conditional density of X일 때, Bayes' theorem을 쓰면 $Pr(G = k | X = x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l}$ 가 된다. 

그래서, classify를 하기 위한 기능으로서 본다면, $f_k(x)$만 알고 있는 것이 $Pr(G = k | X = x)$라는 값을 갖고 있는 것과 거의 같은 것이다.

($\pi_k$라는 likelihood는 $\frac{해당 class = k 데이터 개수}{전체 데이터 개수}$로 쉽게 구할 수 있음.)

자, class density $f_k(x)$를 multivariate Gaussian이라고 두자. 그렇다면 $f_k(x) = \frac{1}{(2\pi)^{p/2}{\left |\Sigma_k  \right |}^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}$ 인데, 여기서 LDA라고 하는 건 $\Sigma_k = \Sigma$라는 special case를 가정했을 때 가능하다. 

이렇게 가정하고 났을 때, 두 개의 classes $k$와 $l$에 대해 비교를 한다고 할 때, log-ratio를 보는 것만으로도 충분하다. 그 값은 $$
log\frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} = log\frac{f_k(x)}{f_l(x)} + log\frac{\pi_k}{\pi_l}
$$이고, 이걸 x에 대해서 정리를 해보면, $log\frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T\Sigma^{-1}(\mu_k - \mu_l)+x^T\Sigma^{-1}(\mu_k-\mu_l)$로써, 마지막 항을 보면 알 수 있듯이 **equation linear in x**라는 것을 알 수 있다. $\Sigma_k$라는 specific matrix를 사용하는 것 대신, common covariance matrix $\Sigma$를 사용하기로 했기 때문에, 약분이 잘 되어서 저렇게 깔끔하게 나오는 것을 확인할 수 있다. 

(반대로 common covariance matrix $\Sigma$ 대신에 specific matrix $\Sigma_k$를 쓰기로 한다면 위의 log-ratio가 약분이 안 되고 **x에 대한 quadratic form**으로 나오기 때문에, 이걸 Quadratic Discriminant Analysis, QDA라고 부른다.)

이렇게 log-ratio function이 linear한 것은 무엇을 imply하느냐?

**class k와 class l간의 decision boundary가 linear in x**라는 것을 의미한다. 즉, p dimension에서의 hyperplane.
물론 이건 어떠한 2개의 classes간에 다 성립하기 때문에, 모든 decision boundaries는 linear하다.
즉, $\mathbb{R}^p$를 class 1, class 2, ...라는 regions로 나누었을 때, 이 regions는 hyperplanes로 seperated될 것이다. 

#![Figure 1](Figure 4.5.png)

이 Figure 1은 p = 2, 3 classes일 때의 idealized example이다. 여기에서 데이터들은 common covariance matrix를 가진 3개의 Gaussian distributions이다. 95% highest probability density로 contour를 그려주고, class centroids도 + 모양으로 넣었다.

여기서 주목할 점으로는, decision boundaries가 centroids를 연결하는 perpendicular line segments가 아니라는 것.

만약 covariance matrix $\Sigma$가 $\sigma^2I$이고, class priors가 equal(i.e. $\pi_k = \frac{1}{3} for k = 1,2,3$)이라면 맞을 것.
