---
# $\SS4.3 Linear Discriminant Analysis, LDA
---
Section 2.4에서 했던 것처럼, classification을 위한 decision theory에서 말하는 건 그거다.

Optimal classification을 위해서는 class posteriors $Pr(G|X)$를 알아야 한다는 것. 
이 $Pr(G|X)$라는 건, $Pr(G = k | X = x)$로, $x$가 주어졌을 때, 어떠한 class $G = k$ 로 classify할 것이냐라는 뜻이다. 

class $k$에 대한 prior가 $\pi_k$ 로 주어져 있고, $f_k(x)$ 가 class k에 대한 class-conditional density of X일 때, Bayes' theorem을 쓰면 $Pr(G = k | X = x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l}$ 가 된다. 

그래서, classify를 하기 위한 기능으로서 본다면, $f_k(x)$만 알고 있는 것이 $Pr(G = k | X = x)$라는 값을 갖고 있는 것과 거의 같은 것이다.

($\pi_k$라는 likelihood는 $\frac{해당 class = k 데이터 개수}{전체 데이터 개수}$로 쉽게 구할 수 있음.)

자, class density $f_k(x)$를 multivariate Gaussian이라고 두자. 그렇다면 $f_k(x) = \frac{1}{(2\pi)^{p/2}{\left |\Sigma_k  \right |}^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}$ 인데, 여기서 LDA라고 하는 건 $\Sigma_k = \Sigma$라는 special case를 가정했을 때 가능하다. 

이렇게 가정하고 났을 때, 두 개의 classes $k$와 $l$에 대해 비교를 한다고 할 때, log-ratio를 보는 것만으로도 충분하다. 그 값은 $$
log\frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} = log\frac{f_k(x)}{f_l(x)} + log\frac{\pi_k}{\pi_l} \cdots (4.9)
$$이고, 이걸 x에 대해서 정리를 해보면, $log\frac{\pi_k}{\pi_l} - \frac{1}{2}(\mu_k + \mu_l)^T\Sigma^{-1}(\mu_k - \mu_l)+x^T\Sigma^{-1}(\mu_k-\mu_l)$로써, 마지막 항을 보면 알 수 있듯이 **equation linear in x**라는 것을 알 수 있다. $\Sigma_k$라는 specific matrix를 사용하는 것 대신, common covariance matrix $\Sigma$를 사용하기로 했기 때문에, 약분이 잘 되어서 저렇게 깔끔하게 나오는 것을 확인할 수 있다. 

(반대로 common covariance matrix $\Sigma$ 대신에 specific matrix $\Sigma_k$를 쓰기로 한다면 위의 log-ratio가 약분이 안 되고 **x에 대한 quadratic form**으로 나오기 때문에, 이걸 Quadratic Discriminant Analysis, QDA라고 부른다.)

이렇게 log-ratio function이 linear한 것은 무엇을 imply하느냐?

**class k와 class l간의 decision boundary가 linear in x**라는 것을 의미한다. 즉, p dimension에서의 hyperplane.
물론 이건 어떠한 2개의 classes간에 다 성립하기 때문에, 모든 decision boundaries는 linear하다.
즉, $\mathbb{R}^p$를 class 1, class 2, ...라는 regions로 나누었을 때, 이 regions는 hyperplanes로 seperated될 것이다. 

#![Figure 1](Figure 4.5.png)

이 Figure 1은 p = 2, 3 classes일 때의 idealized example이다. 여기에서 데이터들은 common covariance matrix를 가진 3개의 Gaussian distributions이다. 95% highest probability density로 contour를 그려주고, class centroids도 + 모양으로 넣었다.

여기서 주목할 점으로는, decision boundaries가 centroids를 연결하는 perpendicular line segments가 아니라는 것.

만약 covariance matrix $\Sigma$가 $\sigma^2I$이고, class priors가 equal(i.e. $\pi_k = \frac{1}{3} for k = 1,2,3$)이라면 맞을 것. ** 이 부분 꼭 프로그램으로 구현해야겠는데...이게 되어야 나중에 Figure 4.11도 가능해짐**

식 (4.9)를 좀 더 자세히 봐보자. 쉽게 생각해보면 된다. $\frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} = 1$일 때가 descision boundary가 되는 것이고, $\frac{Pr(G = k|X = x)}{Pr(G = l|X = x)} > 1$ 이 되면 k class로 classify, 반대의 경우에는 l class로 classify다.

식 (4.9)에서는 앞에 $log$가 붙었기 때문에 이제 이 값이 0이냐 아니냐를 기준으로 따지게 되는 것.

자 그런데 식 (4.9)는 2가지 classes에서만 가지고 log-ratio를 비교해본 것이고, 식 (4.10)처럼 써서 여러가지 classes에 대해 비교할 수 있도록 해볼 수 있다.

$\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+log\pi_k \cdots (4.10)$

그리고 나면, $G(x) = argmax_k\delta_k(x)$라는 decision rule로 쓸 수 있다. 이게 식 (4.9)와 같다는 건, $\delta_k(x) > \delta_l(x)$를 풀어서 써보면 같다는 걸 볼 수 있다.

그런데, practice에서는, $$$\hat{\pi}_k, $$\hat{\mu}_k, $$\hat{\Sigma}$ 값들을 모르기 때문에 estimate값을 써야한다.
$$$\hat{\pi}_k = N_k/N$, $N$은 전체 observation 수, $N_k$는 해당 클래스 k observation 수
$$$\hat{\mu}_k = \sum_{g_i = k}x_i/N_k$, 즉, 해당 클래스 k의 값들의 합 / 해당 클래스 k observation 수
$$$\hat{\Sigma} = \sum_{k = 1}^{K}\sum_{g_i = k}(x_i - $$\hat\mu_k)(x_i - $$\hat\mu_k)^T/(N-K)$ 이렇게 common covariance matrix를 구한다. 

Figure 1에서 보이는 3개의 Gaussian distributions에서 sample size of 30을 각각 뽑아 estimated decision boundaries를 그린 것이 Figure 2이다.

#![Figure 2](Figure 4.5.2.png)

2개의 classes만 존재하는 경우에는, LDA와 linear regression으로 하는 classification간에 관련성이 있다. 2개의 classes를 +1, -1로 코드한다면, least squares로 얻은 coefficient vector와 LDA direction가 proportional하다라는 걸 보일 수 있다. (사실은 +1, -1이 아닌 다른 distinct한 값으로 코딩해도 됨. proportional한 걸 보일 수 있음.) 그렇지만 $N_1 = N_2$일 경우에만 그렇고 아니라면 intercepts가 달라져서 아닐 것.


<details>
<summary>증명</summary>
<div markdown = "1">
증명 어쩌구 저쩌구
</details>

2 classes의 경우에는 LDA와 linear regression으로 한 것과의 관련성이 중요한 건 아니고, **LDA direction을 least squares로 얻는데 있어서는 Gaussian assumption을 전혀 이용하지 않았다는 것이 중요하다!** 그래서 이 LDA의 적용이 꼭 Gaussian data에 대해서만 해야하는 것은 아니다!!! 그런데 식 (4.11)을 보면 intercept 부분이 Gaussian assumption을 필요로 함. 이게 문제 식(4.11)도 추가를 해야되네 씨불
























