---
title: "02 Overview of Supervised Learning"
output: 
  html_document:
    keep_md: true
    
---

#![](cover1.png)

## Introduction

?†µê³„ì  ì§€?„ ?•™?Šµ?˜ ê¶ê·¹?  ëª©ì ??€ **input** (predictors, independent variables) ?“¤?„ ?‚¬?š©?•˜?—¬ **output** (responses, dependent variables)?“¤?˜ ê°’ì„ ?˜ˆì¸¡í•˜?Š” ê²ƒì´?‹¤.

$$Y=f(X)+\epsilon$$

Matrix Notation ?˜ ê²½ìš° $N\times{p}$ matrix $\boldsymbol{X}$ ë¡? ?‚˜??€?‚´ê²? ?œ?‹¤. ëª¨ë“  ë²¡í„°?“¤??€ ?—´ë²¡í„°?“¤ë¡? ê°€? •?˜ê¸? ?•Œë¬¸ì— ?–‰? ¬ $\boldsymbol{X}$ ?˜ ië²ˆì§¸ ?–‰??€ ${x_i}^T$ ë¡? ?‚˜??€?‚¸?‹¤.

ì§€?„?•™?Šµ?—?„œ?˜ task?Š” ì£¼ì–´ì§? ?ž…? ¥ ë²¡í„° $X$ ?— ??€?•œ ì¶œë ¥ê°? $Y$ ?— ??€?•œ ì¢‹ì?€ ?˜ˆì¸¡ê°’ $\widehat{Y}$ ?„ ?–»?Š” ê²ƒì´?‹¤.

?šŒê·€ ë¬¸ì œê°€ ?•„?‹Œ ë¶„ë¥˜ ë¬¸ì œ?—?„œ ?´ì§? ë¶„ë¥˜ë¥? ?•œ?‹¤ê³? ?•  ?•Œ ë²”ì£¼?— ??€?•œ ?˜ˆì¸¡ê°’ $\widehat{G}$ ë¥? ?•´?‹¹ ë²”ì£¼?— ?†?•  ?™•ë¥? $\hat{y}$ ?˜ ê°’ì´ 0.5 ë³´ë‹¤ ?¬?ƒ ?•„?‹ˆ?ƒë¡? ê²°ì •?•œ?‹¤. ?´?Ÿ¬?•œ ? ‘ê·? ë°©ì‹??€ $K$-level ë¶„ë¥˜?—?„ ? ?š©?œ?‹¤. 

?š°ë¦¬ëŠ” ?˜ˆì¸? ê·œì¹™?„ ?„¤ê³„í•˜ê¸? ?œ„?•œ $N$ ê°œì˜ **training data** $(x_i,y_i)$ ?˜?Š” $(x_i,g_i)$ , $i=1,...,N$ ê°€ ?•„?š”?•˜?‹¤.


## Two Simple Approaches to Prediction : Least Squares and Nearest Neighbors

?‘ ê°€ì§€ ?‹¨?ˆœ?•˜ì§€ë§? ê°•ë ¥?•œ ?˜ˆì¸? ë°©ë²•ë¡ ì´?‹¤. ë°”ë¡œ least squares ?— ?˜?•œ ?„ ?˜• ëª¨ë¸ ? ?•©ê³? $k$-nearest neighbor ?˜ˆì¸? ê·œì¹™?´?‹¤. 

?„ ?˜• ëª¨ë¸??€ êµ¬ì¡°?— ??€?•œ ê°•ë ¥?•œ ê°€? •?´ ì¡´ìž¬?•˜ê³? ?•ˆ? •? ?´ì§€ë§? ë¶€? •?™•?•œ ?˜ˆì¸? ê²°ê³¼ë¥? ? œê³µí•œ?‹¤. $k$-nearest neighbor ë°©ë²•??€ êµ¬ì¡°?— ??€?•œ ?œ ?—°?•œ ê°€? •?´ ì¡´ìž¬?•˜ë©? ì¢…ì¢… ? •?™•?•˜ì§€ë§? ë¶ˆì•ˆ? •?•œ ?˜ˆì¸? ê²°ê³¼ë¥? ? œê³µí•œ?‹¤.

### Linear Models and Least Sqaures

ì£¼ì–´ì§? ?ž…? ¥ ë²¡í„° ?˜¹??€ ? „ì¹? ?–‰? ¬ ($X$ ?Š” ?—´ë²¡í„°) $X^T=(X_1,X_2,...,X_p)(n\times{p})$ ê°€ ?žˆ?„ ?•Œ, ?š°ë¦¬ëŠ” ì¶œë ¥ê°? $Y$ ë¥? ?•„?ž˜?˜ ëª¨ë¸?„ ?†µ?•´ ?˜ˆì¸¡í•œ?‹¤.

$$\widehat{Y}=\hat{B_0}+\sum_{j=1}^{p}{X_j\hat{\beta_j}}$$

ë³´í†µ 1ë²¡í„°ë¥? $X$ ?— ?¬?•¨?‹œì¼? ? ˆ?Ž¸ ì¶”ì •ì¹? $\hat{\beta_0}$ ë¥? ê³„ìˆ˜ ?–‰? ¬ $\hat{\beta}$ ?— ?¬?•¨?‹œ?‚¤?Š” ê²ƒì´ ?Ž¸ë¦¬í•˜ë©?, ?„ ?˜• ëª¨ë¸?„ ?‚´? ?˜ ê²°ê³¼?¸ ë²¡í„° ?˜•?ƒœë¡? ?•„?ž˜??€ ê°™ì´ ?‘œ?˜„?•  ?ˆ˜ ?žˆ?‹¤.

$$\widehat{Y}_{(n\times{1})}=X^T_{(n\times{p})}\hat{\beta}_{(p\times{1})}$$

?¼ë°˜ì ?œ¼ë¡? $\widehat{Y}$ ê°€ $K$ ë²¡í„°?¼ê³? ?•˜ë©? $\beta$ ?Š” $p\times{K}$ ?˜ ê³„ìˆ˜ ?–‰? ¬?´ ?  ê²ƒì´?‹¤.

? ?•©?œ ì§ì„  $(X,\hat{Y})$??€ $p+1$-ì°¨ì›?˜ ?ž…ì¶œë ¥ ê³µê°„?—?„œ?˜ hyperplane?„ ?‚˜??€?‚´ê²? ?œ?‹¤. (?‹¨?ˆœ?„ ?˜•?šŒê·€?—?„œ ?šŒê·€ ì§ì„ ?´ 1ì°¨ì›?´?“¯?´)

> $X$ ?— ?ƒ?ˆ˜ê°€ ?¬?•¨?˜?–´?žˆ?„ ê²½ìš°, hyperplane??€ ?›? ?„ ?¬?•¨?•˜ì§€ ?•Š?Š” subspace?´?‹¤. ?´?Š” $Y$-ì¶•ì„ $(0,\hat{\beta_0})$ ?—?„œ ?žë¥´ëŠ” affine set?´?‹¤. (affine space?Š” ?›? ?´ ?—†?‹¤)

**ì§€ê¸ˆë?€?„° ?š°ë¦¬ëŠ” ? ˆ?Ž¸?´ $\widehat{\beta}$ ?— ?¬?•¨?˜?–´ ?žˆ?‹¤ê³? ê°€? •?•œ?‹¤.**

?š°ë¦¬ëŠ” ì£¼ì–´ì§? training dataë¡? ?–´?–»ê²? ?„ ?˜• ëª¨ë¸?„ ? ?•©?•  ?ˆ˜ ?žˆ?„ê¹?? ?‹¤?–‘?•œ ë°©ë²•?“¤?´ ì¡´ìž¬?•˜ì§€ë§?, ê°€?ž¥ ?•Œ? ¤ì§? ë°©ë²•?œ¼ë¡? **least square** ë°©ë²•?´ ?žˆ?‹¤. ?´ ë°©ë²•?—?„œ ?š°ë¦¬ëŠ” residual sum of squares (RSS) ë¥? ìµœì†Œ?™”?•˜?Š” ?šŒê·€ ê³„ìˆ˜ $\beta$ ë¥? ì°¾ëŠ”?‹¤.

$$\text{RSS}(\beta)=\sum_{i=1}^{N}(y_i-x_i^T\beta)^2$$

$\text{RSS}(\beta)$ ?Š” ëª¨ìˆ˜?“¤?˜ quadratic function (2ì°? ?•¨?ˆ˜) ?´ë¯€ë¡? ìµœì†Ÿê°’ì´ ?•­?ƒ ì¡´ìž¬?•˜ì§€ë§?, unique ?•˜ì§€?Š” ?•Š?„ ?ˆ˜ ?žˆ?‹¤. ?œ„ ?´ì°¨ì‹?˜ ?•´?Š” matrix notation?œ¼ë¡? characterize?•˜?Š” ê²ƒì´ ê°€?ž¥ ?‰½?‹¤. 

$$\text{RSS}(\beta)=(\boldsymbol{y}-\boldsymbol{X}\beta)^T(\boldsymbol{y}-\boldsymbol{X}\beta)$$

?´ë¥? $\beta$ ?— ??€?•´ ë¯¸ë¶„?•˜ë©? ?•„?ž˜?˜ **normal equation** ?„ ?–»?„ ?ˆ˜ ?žˆê³?

$$\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\beta)=0$$

$\boldsymbol{X^TX}$ ê°€ non-singular ?–‰? ¬?´ë©? (?—­?–‰? ¬?´ ì¡´ìž¬?•˜ë©?), unique solution?„ ?•„?ž˜??€ ê°™ì´ ?–»?„ ?ˆ˜ ?žˆ?‹¤.

$$\widehat{\beta}=(\boldsymbol{X^TX})^{-1}\boldsymbol{X^Ty}$$

? „ì²? fitted surfaceë¥? $p$ ê°œì˜ ëª¨ìˆ˜?¸ $\hat{\beta}$ ?“¤ë¡? ?‚˜??€?‚¼ ?ˆ˜ ?žˆ?Š” ê²ƒì—?„œ ì§ê?€? ?œ¼ë¡? ëª¨ë¸?„ ? ?•©?•˜?Š” ?°?— ë§¤ìš° ë§Žì?€ ?žë£Œê?€ ?•„?š”?•˜ì§€ ?•Š?‹¤?Š” ê²ƒì„ ?•Œ ?ˆ˜ ?žˆ?‹¤.

**ë¶„ë¥˜ ë¬¸ì œ** ?—?„œ?˜ ?„ ?˜• ëª¨ë¸?— ??€?•´ ?ƒê°í•´ë³´ìž. 

ê´€ì¸¡ì¹˜ê°€ ?‚˜??€?‚´?Š” ?ŒŒ??€?ƒ‰ (0), ?˜¤? Œì§€?ƒ‰ (1) ?‘ ê°œì˜ ?ƒ‰?— ??€?•œ ?´ì§? ë¶„ë¥˜?¼ê³? ?–ˆ?„ ?•Œ, ë²”ì£¼?— ?†?•  ?˜ˆì¸? ?™•ë¥? 0.5 ì´ˆê³¼ë©? ?˜¤? Œì§€, ?´?•˜ë©? ?ŒŒ??€?ƒ‰?œ¼ë¡? ë¶„ë¥˜?•˜ê²? ?œ?‹¤. ?´ ?•Œ?˜ hyperplane, ì¦? **decision boundary** ?Š” (?´ ê²½ìš° ?„ ?˜•) $[x:x^T\hat{\beta}=0.5]$ ?œ¼ë¡? ?‚˜??€?‚˜ê²? ?œ?‹¤. 

?´ ê²½ìš°?—?„ ?š°ë¦¬ëŠ” ?—¬?Ÿ¬ ê°œì˜ ?˜¤ë¶„ë¥˜ ê²°ê³¼?“¤?„ ?™•?¸?•  ?ˆ˜ ?žˆ?Š”?°, ?„¤ê³„ëœ ?žë£Œì˜ ?ƒ?„± ê³¼ì •?— ??€?•´ ?´?•¼ê¸°í•˜ì§€ ?•Š??€ ?ƒ?ƒœ?—?„œ ?‘ê°€ì§€ ê°€?Š¥?•œ ?‹œ?‚˜ë¦¬ì˜¤ë¥? ?ƒê°í•´ë³´ìž.

* **Scenario 1:** ê°? ë²”ì£¼?— ?†?•˜?Š” training dataê°€ ëª¨í‰ê· ì´ ?‹¤ë¥? ?…ë¦½ì¸ bivariate normal distribution ?—?„œ ?ƒ?„±?˜?—ˆ?‹¤.

### Simulation

$$\text{Blue~}N_2(\begin{bmatrix}0\\1\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix})$$

$$\text{Orange~}N_2(\begin{bmatrix}1\\0\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix})$$

```{r}
library(tidyverse)
set.seed(2013122044)
## scenario 1 example
# independent bivariate normal distribution

# blue = N_2([0,1],[1,0,0,1])
blue=data.frame(X1=rnorm(100,0,1),X2=rnorm(100,1,1))
# orange = N_2([1,0],[1,0,0,1])
orange=data.frame(X1=rnorm(100,1,1),X2=rnorm(100,0,1))
# Y
Y=factor(c(rep('Blue',100),rep('Orange',100)))
# data
df=data.frame(cbind(rbind(blue,orange),Y))
# visualization
ggplot(df,aes(x=X1,y=X2))+
  geom_point(aes(color=Y))+
  scale_colour_manual(values=c('dodgerblue','orange'))+
  ggtitle('Visualization of Scenario 1')+
  theme_bw()+
  theme(plot.title=element_text(face='bold'))
  
```


* **Scenario 2:** ê°? ë²”ì£¼?— ?†?•˜?Š” training dataê°€ ê°? ë¶„í¬?˜ ëª¨í‰ê·? ?žì²´ê?€ normal distribution?„ ?”°ë¥´ëŠ” 10ê°œì˜ low variance normal distribution?˜ mixture?—?„œ ?ƒ?„±?˜?—ˆ?‹¤. (Gaussian Mixture)

$$\mu_{blue}\text{~}N_2(\begin{bmatrix}1\\0\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix})$$

$$\mu_{orange}\text{~}N_2(\begin{bmatrix}0\\1\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix})$$

```{r}
## 10 Gaussian Mixture Example
# generative model
generator=function(n){
  # parameters for bivariate mean distributions to generate 10 bivariate mean vectors
  n_mixture=10
  # additional paramters for class distributions
  sample_sd=sqrt(1/5)
  # gernerate 10 mean vectors for each class
  blue_mean_sample=data.frame(X1=rnorm(n_mixture,1,1),X2=rnorm(n_mixture,0,1))
  orange_mean_sample=data.frame(X1=rnorm(n_mixture,0,1),X2=rnorm(n_mixture,1,1))
  # blue
  idx1=sample(10,1)
  sample_blue=data.frame(X1=rnorm(n,blue_mean_sample[idx1,][[1]],sample_sd),X2=rnorm(n,blue_mean_sample[idx1,][[2]],sample_sd))
  # orange
  idx2=sample(10,1)
  sample_orange=data.frame(X1=rnorm(n,orange_mean_sample[idx2,][[1]],sample_sd),X2=rnorm(n,orange_mean_sample[idx2,][[2]],sample_sd))
  
  # final samples
  X=rbind(sample_blue,sample_orange)
  Y=factor(c(rep('Blue',100),rep('Orange',100)))
  df=data.frame(X,Y)
  return (df)
}
```


```{r}
# visualization for 100 samples for each class
set.seed(2013122044)
df=generator(100)
ggplot(df,aes(x=X1,y=X2))+
  geom_point(aes(color=Y))+
  scale_colour_manual(values=c('dodgerblue','orange'))+
  ggtitle('Visualization of Scenario 2')+
  theme_bw()+
  theme(plot.title=element_text(face='bold'))
```

> Gaussian Mixture??€?

Gaussian Mixture?Š” generative modelë¡? ê°€?ž¥ ?ž˜ ?„¤ëª…ë  ?ˆ˜ ?žˆ?‹¤. ë¨¼ì?€ ?–´?–¤ Gaussian Component (ex. 10 Mixture of Bivariate Normal?´ë©? 10ê°? ì¤? ?–´?–¤ Bivariate Normal Random Variable?„ ?‚¬?š©?• ì§€) ë¥? ?‚¬?š©?•  ì§€ë¥? ê²°ì •?•˜?Š” ?´?‚°?˜• ë³€?ˆ˜ë¥? ?ƒ?„±?•˜ê³? ê²°ì •?œ ë°€?„?—?„œ ê´€ì¸¡ì¹˜ë¥? ?ƒ?„±?•˜ê²? ?œ?‹¤. 

ê°? ë²”ì£¼?— ?†?•˜?Š” ê´€ì¸¡ì¹˜ê°€ ?•œ ê°œì˜ Normal Random Variable?„ ê°–ëŠ”?‹¤ë©? ?„ ?˜•?˜ decision boundaryê°€ ìµœì ?´ë©? ê²¹ì¹˜?Š” ì§€?—­??€ ë¶ˆê?€?”¼?•˜ë©? ?˜ˆì¸¡ë˜?–´?•¼ ?•˜?Š” ë¯¸ëž˜ ?žë£? ?˜?•œ ?´?Ÿ¬?•œ ê²¹ì¹˜?Š” ì§€?—­?—?„œ ?ž?œ ë¡œìš¸ ?ˆ˜ ?—†?„ ê²ƒì´?‹¤. 

ê·¸ë ‡?‹¤ë©? ê°? ë²”ì£¼?— ?†?•˜?Š” ê´€ì¸¡ì¹˜ê°€ ?—¬?Ÿ¬ ê°œì˜ ì¢ê²Œ ëª¨ì—¬ êµ°ì§‘?„ ?´ë£¨ëŠ” Normal Random Variable?“¤?„ ê°–ëŠ”?‹¤ë©? ?´?•¼ê¸°ê?€ ?‹¤ë¥´ë‹¤. ìµœì ?˜ decision boundary?Š” ë¹„ì„ ?˜•?´ë©? disjoint?•˜ê³? ë³´ë‹¤ ?–»ê¸? ?–´? ¤?š¸ ê²ƒì´?‹¤.

### Nearest-Neighbor Methods

ìµœê·¼? ‘ ?´?›ƒ ë°©ë²•??€ ?ž…? ¥ ê³µê°„ ?‚´?— ì¡´ìž¬?•˜?Š” $x$ ??€ ê°€?ž¥ ê°€ê¹Œìš´ training set $T$ ?— ?žˆ?Š” ê´€ì¸¡ì¹˜?“¤?„ ?‚¬?š©?•˜?—¬ $\widehat{Y}$ ë¥? ?˜•?„±?•œ?‹¤.

$$\widehat{Y}(x)=\frac{1}{k}\sum_{x_i\in{N_k(x)}}y_i$$

$N_k(x)$ ?Š” $x$ ?˜ $k$ ê°œì˜ ìµœê·¼? ‘ training data?“¤ë¡? ? •?˜?œ?‹¤. ?š°ë¦¬ëŠ” ?—¬ê¸°ì„œ ê±°ë¦¬ë¥? ? •?˜?•´?•¼?•˜ë©? Euclidean distanceë¥? ê°€? •?•œ?‹¤. 

$$L_2\text{-norm}=\sqrt{\sum_{i=1}^{n}(x_1-x_2)^2}$$

> ì¦?, ê°€?ž¥ ê°€ê¹Œìš´ kê°œì˜ ?›ˆ? ¨ ?°?´?„°?“¤?˜ ì¢…ì†ë³€?ˆ˜ ê°’ë“¤?„ ?‰ê· ë‚´?–´ ì¶”ì •?•˜?Š” ê²?

$K$-ìµœê·¼? ‘ ?´?›ƒ ? ?•©?—?„œ training data?— ??€?•œ ?˜¤ì°¨ëŠ” $k$?— ??€?•´ ì¦ê?€?•¨?ˆ˜?´ë©?, $k=1$ ?¸ ê²½ìš° ?•­?ƒ 0?¼ ê²ƒì´?‹¤. 

$K$-ìµœê·¼? ‘ ?´?›ƒ ? ?•©??€ $p$ê°œì˜ ëª¨ìˆ˜?“¤?„ ê°€ì§€?˜ least-square??€ ?‹¤ë¥´ê²Œ $k$?˜ ê°’ë§Œ?´ ê²°ì •?•œ?‹¤. 

?š°ë¦¬ëŠ” **?š¨ê³¼ì ?¸** $k$?˜ ê°’ì´ $N/k$ ?´ë©? ?¼ë°˜ì ?œ¼ë¡? $p$ ë³´ë‹¤ ?¬ê³?, $k$ ?˜ ê°’ì´ ì¦ê?€?•¨?— ?”°?¼ ê°ì†Œ?•˜?Š” ê²ƒì„ ?™•?¸?•  ?ˆ˜ ?žˆ?‹¤.

> ?´?œ ?— ??€?•´?„œ?Š”, ?´?›ƒ?“¤?´ ê²¹ì¹˜ì§€ ?•Š?Š”?‹¤ë©?, $N/k$ ê°œì˜ ?´?›ƒì§‘ë‹¨?“¤?´ ?žˆ?„ ê²ƒì´ê³? ?š°ë¦¬ëŠ” ê°? ?´?›ƒì§‘ë‹¨?— ??€?•´ ?•œ ê°œì˜ ëª¨ìˆ˜ (a mean) ë¥? ? ?•©?•˜ê²? ?œ?‹¤.

?˜?•œ ?š°ë¦¬ê?€ training set?—?„œ $k$ ë¥? ?„ ?ƒ?•˜?Š” ê¸°ì?€?œ¼ë¡? sum of squared errorë¥? ?‚¬?š©?•  ?ˆ˜ ?—†?‹¤?Š” ê²? ?˜?•œ ?žëª…í•˜?‹¤ (?•­?ƒ $k=1$ ?„ ?„ ?ƒ?•˜ê²? ?  ê²ƒì´ë¯€ë¡?)

?”°?¼?„œ $K$-ìµœê·¼? ‘ ?´?›ƒ ë°©ë²•??€ ?œ„?—?„œ ?†Œê°œëœ ?‹œ?‚˜ë¦¬ì˜¤ ì¤? **2ë²ˆì§¸ ?‹œ?‚˜ë¦¬ì˜¤**?— ë³´ë‹¤ ? ?•©?•  ê²ƒì´?‹¤. (? •ê·? ë¶„í¬ë¥? ?”°ë¥´ëŠ” ?žë£Œì— ??€?•´ decision boundaryê°€ ?•„?š” ?´?ƒ?œ¼ë¡? noisy?•  ê²ƒì´ê¸? ?•Œë¬¸ì—)

?†Œê°œëœ ?‘ ë°©ë²•?— ??€?•´ **Least Square** ê°™ì?€ ê²½ìš° ? ?•© ëª¨ë¸(model or decision boundary) ?´ **Low Variance** ê·¸ë¦¬ê³? **High Bias** ë¥? ê°–ê³  **K-ìµœê·¼? ‘ ?´?›ƒ** ê°™ì?€ ê²½ìš° **High Variance** ê·¸ë¦¬ê³? **High Bias**ë¥? ê°–ëŠ”?‹¤. 

ì±…ì—?„œ?Š” Train ê·¸ë¦ê³? Test Set?—?„œ?˜ Error Rate??€ Optimal Bayes Error Rateê°€ $k$ ?— ?”°?¼ ?–´?–»ê²? ?‚˜??€?‚˜ê²? ?˜?Š”ì§€ ë³´ì—¬ì£¼ê³  ?žˆ?‹¤.

#![](knn.PNG)

**?˜„?ž¬ ?‚¬?š©?˜ê³? ?žˆ?Š” ë§Žì?€ ?œ ëª…í•œ ê¸°ë²•?“¤??€ ?´ ?‘ ê°€ì§€ ?‹¨?ˆœ?•œ ? ˆì°¨ë“¤?˜ ë³€?˜•?´?‹¤.** ?‹¤? œë¡? 1-ìµœê·¼? ‘ ?´?›ƒ ë°©ë²•??€ ??€ì°¨ì› ë¬¸ì œ?—?„œ?˜ ë§Žì?€ ë¶€ë¶„ì—?„œ ?‚¬?š©?˜ê³? ?žˆ?‹¤. 

?•„?ž˜?˜ ë°©ë²•?“¤?´ ?´ ?‹¨?ˆœ?•œ ? ˆì°¨ë“¤?´ ?‘?š©?˜ê³? ê°œì„ ?œ ?˜ˆ?´?‹¤.

* **Kernel Method**?Š” ëª©í‘œ ì§€? ê¹Œì?€?˜ ê±°ë¦¬ê°€ ê°ì†Œ?•  ?ˆ˜ë¡? 0?— ê°€ê¹Œì›Œì§€?Š” ê°€ì¤‘ì¹˜ë¥? ?‚¬?š©?•œ?‹¤.

* ê³ ì°¨?› ê³µê°„?—?„œ distance kernel??€ ?Š¹? • ë³€?ˆ˜?“¤?„ ê°•ì¡°?•˜ê¸? ?œ„?•´ ë³€?˜•?œ?‹¤.

* **Local Regression** ??€ ?„ ?˜• ëª¨ë¸?„ locally weighted least squaresë¥? ?™œ?š©?•˜?—¬ ? ?•©?•œ?‹¤.

* ?„ ?˜• ëª¨ë¸?„ ?›?ž˜?˜ ?ž…? ¥ê°’ì˜ **Basis Expansion**?œ¼ë¡œì¨ ?ž„?˜?˜ ë³µìž¡?•œ ëª¨ë¸ë¡œì˜ ? ?•©?´ ê°€?Š¥?•˜ê²? ?•œ?‹¤.

* **Projection Pursuit** ê·¸ë¦¬ê³? **Neural Network** ëª¨ë¸?“¤??€ ë¹„ì„ ?˜•?œ¼ë¡? ë³€?™˜?œ ?„ ?˜• ëª¨ë¸?“¤?˜ ?•©?œ¼ë¡? ?´ë£¨ì–´ì§„ë‹¤.

## Statistical Decision Theory

$X\in{R^p}$ ë¥? ?‹¤?ˆ˜ ?™•ë¥? ?ž…? ¥ ë²¡í„°?¼ê³? ?•˜ê³? $Y\in{R}$ ë¥? ?‹¤?ˆ˜ ?™•ë¥? ì¶œë ¥ ë³€?ˆ˜?¼ê³? ?•˜ë©? ê²°í•© ë¶„í¬ë¥? $\text{Pr}(X,Y)$ ?¼ê³? ?•œ?‹¤ë©?,

?š°ë¦¬ëŠ” ì£¼ì–´ì§? ?ž…? ¥ê°? $X$ ë¥? ê°€ì§€ê³? $Y$ ë¥? ?˜ˆì¸¡í•˜?Š” ?•¨?ˆ˜ $f(X)$ ë¥? ì°¾ê³ ?ž ?•œ?‹¤.

?´?Ÿ¬?•œ ?´ë¡ ì?€ **loss function** $L(Y,f(X))$ ?„ ?†µ?•´ ?˜ˆì¸¡ì—?„œ?˜ ?˜¤ì°¨ë?? penalize?•  ?ˆ˜ ?žˆê³?, ê°€?ž¥ ?„ë¦? ?“°?´ê³? ê°„íŽ¸?•œ ?˜ˆê°€ ë°”ë¡œ **squared error loss** $L(Y,f(X))=(Y-f(X))^2$ ?´?‹¤.

?´?Š” $f$ë¥? ì°¾ê¸° ?œ„?•œ ê¸°ì?€?´ ?  ?ˆ˜ ?žˆ?Š” **Expected (Squared) Prediction Error** ë¥? ê³„ì‚°?•  ?ˆ˜ ?žˆê²? ?•œ?‹¤. 

$$\text{EPE}(f)=E[(Y-f(X))^2]=\int(y-f(x))^2f(x,y)dydx$$

?œ„ ?‹?„ $X$ ?— ??€?•´ conditioning ?•˜ë©? ?š°ë¦¬ëŠ” EPEë¥? ?•„?ž˜??€ ê°™ì´ ?“¸ ?ˆ˜ ?žˆ?‹¤.

$$\text{EPE}(f)=E_{X}E_{Y|X}[(Y-f(X))^2|X]$$

EPEë¥? pointwise?•˜ê²? ìµœì†Œ?™”?•  ?ˆ˜ ?žˆ?Š” $\hat{f}(x)$ ?Š”

$$\hat{f}(x)=\text{argmin}_cE_{Y|X}[(Y-c)^2|X=x]$$

?´ë©?, ?´?— ??€?•œ ?•´?Š” ë°”ë¡œ

$$f(x)=E(Y|X=x)$$ 

?´ë©?, conditional expectation?´ê³?, **regression function** ?´ê¸°ë„ ?•˜?‹¤. ì¦?, ?–´?Š ì§€?  $X=x$ ?—?„œ?“  $Y$ ?— ??€?•œ ìµœì„ ?˜ ?˜ˆì¸¡ì?€ ì¡°ê±´ë¶€ ?‰ê· ì´?‹¤. (ìµœì„ ?˜ ê¸°ì?€?´ average squared error?¼ ?•Œ)

**ìµœê·¼? ‘ ?´?›ƒ** ?˜ ê²½ìš°?—?Š” 

$$\hat{f}(x)=\text{Ave}(y_i|x_i\in{N_k(x)})$$

?´ ?œ?‹¤. ($N_k(x)$ ?Š” training set $T$ ?—?„œ $x$ ê³? ê°€?ž¥ ê°€ê¹Œìš´ $k$ ê°œì˜ ê´€ì¸¡ì¹˜?“¤?˜ ì§‘í•©)

?œ„ ?‹?—?„œ ?‘ ê°€ì§€?˜ **ê·¼ì‚¬** ê°€ ?´ë£¨ì–´ì§€?Š”?°,

* ê¸°ëŒ“ê°’ì´ ?‘œë³? ?°?´?„°?— ??€?•œ ?‰ê· ìœ¼ë¡? ê·¼ì‚¬?œ?‹¤.

* ?•œ ì§€? ?—?„œ conditioning ?•˜?Š” ê²ƒì?€ ëª©í‘œ ì§€? ?— ê°€ê¹Œìš´ ?–´?–¤ ì§€?—­?— ??€?•´ conditioning ?˜?Š” ê²ƒìœ¼ë¡? ?™„?™”?œ?‹¤.

?‘œë³¸ì˜ ?¬ê¸°ê?€ ì»¤ì§ˆ ?ˆ˜ë¡? mild regularity condition?“¤ ?•˜?—?„œ ê·¼ì‚¬?˜?Š” ?„±ì§ˆì„ knn ?˜?•œ ë³´ì—¬ì¤€?‹¤. ?•˜ì§€ë§? ?š°ë¦¬ëŠ” ì¢…ì¢… ?‘œë³? ?¬ê¸°ê?€ ë§¤ìš° ?¬ì§€ ?•Šê³?, ì¶”ê?€? ?œ¼ë¡? **ì°¨ì›** $p$ ê°€ ì»¤ì§ˆ ?ˆ˜ë¡? ë°œìƒ?•˜?Š” ë¬¸ì œ ?˜?•œ ?’¤?—?„œ ?†Œê°œëœ?‹¤.

?‹¤?‹œ ?„ ?˜• ?šŒê·€ ? ?•©?œ¼ë¡? ?Œ?•„ê°€, ìµœì†Œ? œê³±ë²•ê³? $k$-ìµœê·¼? ‘ ?´?›ƒ ë°©ë²• ëª¨ë‘ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì„ ?‰ê· ì„ ?†µ?•´ ê·¼ì‚¬?•˜?Š” ê²ƒì´?¼?Š” ê²ƒì„ ?•Œ ?ˆ˜ ?žˆ?‹¤. ?•˜ì§€ë§?, ëª¨ë¸ ê°€? •?—?„œ ?° ì°¨ì´ê°€ ?žˆ?‹¤.

* Least Squares ?Š” $f(x)$ ê°€ globally linear function?— ?˜?•´ ?ž˜ ê·¼ì‚¬?  ?ˆ˜ ?žˆ?‹¤ê³? ê°€? •?•œ?‹¤. 

* $\beta=(E(XX^T))^{-1}E(XY)$

ë°˜ë©´ 

* $k$-ìµœê·¼? ‘ ?´?›ƒ??€ $f(x)$ ê°€ locally constant function?— ?˜?•´ ?ž˜ ê·¼ì‚¬?  ?ˆ˜ ?žˆ?‹¤ê³? ê°€? •?•œ?‹¤.

ì±…ì—?„œ ?›„?— ?†Œê°œë  ë§Žì?€ ?˜„??€ ê¸°ë²•?“¤??€ Model Based ?´ë©? ?—„ê²©í•œ ?„ ?˜• ëª¨ë¸ë³´ë‹¤ ?›¨?”¬ ?œ ?—°?•˜?‹¤. 

> ? Example of Additive Model

?˜?•œ loss function?—?Š” ?œ„?—?„œ ?´?•¼ê¸°í•œ least square?˜ $L_2$ ë§? ?žˆ?Š” ê²ƒì´ ?•„?‹ˆ?‹¤. ?´ë¥? $L_1$ ?œ¼ë¡? ??€ì²´í•˜ê²? ?  ê²½ìš° $(E[|Y-f(X)|])$ ?•´?Š” $\hat{f}(x)=median(Y|X=x)$ , ì¦? ì¡°ê±´ë¶€ ì¤‘ì•™ê°’ì´ ?œ?‹¤. ?´?Š” location?— ??€?•œ ?‹¤ë¥? measure?´ë©?, ì¡°ê±´ë¶€ ?‰ê· ë³´?‹¤ ?”?š± ë¡œë²„?Š¤?Š¸?•œ ì¶”ì •?Ÿ‰?´?‹¤. ?•˜ì§€ë§? ë¯¸ë¶„?—?„œ?˜ ë¶ˆì—°?† ì§€? ?´ ë°œìƒ?•˜ê¸? ?•Œë¬¸ì— ?ˆ˜ì¹˜ì ?œ¼ë¡? ?„ë¦? ?“°?´ì§€ ?•Š?Š”?‹¤.

**ë²”ì£¼?˜• ì¢…ì† ë³€?ˆ˜** ?˜ ê²½ìš°?—?Š” ê°™ì?€ ê°œë…?œ¼ë¡? ?‹¤ë¥? loss functionë§Œì„ ? •?˜?•´ì£¼ë©´ ?œ?‹¤. ?š°ë¦¬ì˜ loss function??€ $K\times{K}$ ?–‰? ¬ $\boldsymbol{L}$ ë¡? ?‘œ?˜„?  ?ˆ˜ ?žˆê³?, (confusion matrix) $L(k,l)$ ??€ $G_k$ ?— ?†?•˜?Š” ê´€ì¸¡ì¹˜ë¥? $G_l$ ?— ë¶„ë¥˜?–ˆ?„ ?•Œ?˜ cost?´?‹¤. 

ê°€?ž¥ ë§Žì?€ ê²½ìš°?— **zero-one** loss function ?„ ?‚¬?š©?•œ?‹¤.

$$\text{EPE}=E[L(G,\hat{G}(X)]$$

?‹¤?‹œ conditioning ?•˜?—¬ ?•„?ž˜??€ ê°™ì´ EPEë¥? ?‚˜??€?‚¼ ?ˆ˜ ?žˆ?‹¤.

$$EPE=E_X\sum_{k=1}^{K}L(G_k,\hat{G}(X))\text{Pr}(G_k|X)$$

?´ ?—­?‹œ EPEë¥? ?•„?ž˜??€ ê°™ì´ ìµœì†Œ?™”?•˜?Š” $\hat{G}(x)$ ë¥? ?‚˜??€?‚¼ ?ˆ˜ ?žˆê³?,

$$\hat{G}(x)=\text{argmin}_{g\in{G}}\sum_{k=1}^{K}L(G_k,g)\text{Pr}(G_k|X=x)$$

0-1 loss function ?˜ ê²½ìš° ?´?Š” ?•„?ž˜??€ ê°™ì´ ? •ë¦¬ë  ?ˆ˜ ?žˆ?‹¤.

$$\hat{G}(x)=\text{argmin}_{g\in{G}}(1-\text{Pr}(g|X=x))$$

?˜?Š”

$$\hat{G}(x)=G_k\text{ if Pr}(G_k|X=x)=\text{max}_{g\in{G}}\text{Pr}(g|X=x)$$

?œ„?˜ ?•´?Š” **Bayes Classifier** ?¼ê³? ?•˜ë©?, ?´?Š” ?š°ë¦¬ëŠ” ?´?‚°?˜• ì¡°ê±´ë¶€ ë¶„í¬ $\text{Pr}(G|X)$ ë¥? ?‚¬?š©?•˜?—¬ ê°€?ž¥ ?†?•  ?™•ë¥ ì´ ?†’??€ ë²”ì£¼?— ë¶„ë¥˜?•˜?Š” ê²ƒì´?‹¤. ?´?Ÿ¬?•œ Bayes Classifier?˜ ?˜¤ì°¨ìœ¨?„ **Bayes Rate** ?¼ê³? ?•œ?‹¤.

?‹¤?‹œ $k$-NN ?œ¼ë¡? ?Œ?•„??€, ?´ ë°©ë²•?´ ì§ì ‘? ?œ¼ë¡? ?´?Ÿ¬?•œ ?•´?— ê·¼ì‚¬?•˜?Š” ê²ƒì„ ë³´ì¼ ?ˆ˜ ?žˆ?‹¤. (majority vote, ?‹¤?ˆ˜ê²°ë¡œ) 

?•˜ì§€ë§? ?´ ê²½ìš°?—?„ $k$-NN??€ ì§€? ?— ??€?•œ ì¡°ê±´ë¶€ ?™•ë¥ ì´ ì§€? ?˜ ?´?›ƒ?˜ ì¡°ê±´ë¶€ ?™•ë¥ ë¡œ ?™„?™”?˜ë©? ?™•ë¥ ë“¤?´ training-sample proportion?— ?˜?•´ ì¶”ì •?œ?‹¤.


## Local Methods in High Dimensions

KNN averaging ?„ ?†µ?•´ training data ?˜ ?‘œë³? ?¬ê¸°ê?€ ?´?ƒ? ?œ¼ë¡? ì»¤ì§ˆ?ˆ˜ë¡? ?´ë¡ ì ?¸ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì— ê·¼ì‚¬?•  ?ˆ˜ ?žˆ?„ê¹??

?´?Ÿ¬?•œ ? ‘ê·? ë°©ë²•ê³? ì§ê?€??€ **ê³ ì°¨?›** ?—?„œ ì² ì?€?žˆ ë¶€?ˆ´ì§€ë©? ?´ë¥? **ì°¨ì›?˜ ??€ì£?** ?¼ê³? ?•œ?‹¤.

$p$-ì°¨ì›?˜ ?‹¨?œ„ hypercube (?…Œ?„œ? ‰?Š¸, ëª¨ë“  ë³€?˜ ê¸¸ì´ê°€ ê°™ì?€ ?„?˜•) ?— ê· ë“±?•˜ê²? ë¶„í¬?˜?–´ ?žˆ?Š” ?ž…? ¥ê°’ì— ??€?•œ ìµœê·¼? ‘ ?´?›ƒ ? ˆì°¨ë?? ?ƒê°í•´ë³´ìž. ?š°ë¦¬ê?€ $r/N$ ë§Œí¼?„ ?ž¡?•„?‚´ê¸? ?œ„?•œ ëª©í‘œ ì§€? ?— ??€?•œ hypercubical ?´?›ƒ ì§‘ë‹¨?„ ë³´ë‚¸?‹¤ê³? ?•˜ë©?, 

?´?Š” ê²°êµ­ $\frac{r}{\text{?‹¨?œ„ ë¶€?”¼}}$ ?´ë¯€ë¡? ê¸°ëŒ“ë³€?˜ ê¸¸ì´?Š”  $e_p(r)=r^{1/p}$ ê°€ ?  ê²ƒì´?‹¤. ì¦?, 10ì°¨ì› ?ž…? ¥ ê³µê°„?—?„œ?Š” $e_{10}(0.01)=0.63$ , $e_{10}(0.1)=0.8$ ?´?‹¤. 

ì¦?, ?´?Š” ? „ì²? ?žë£Œì˜ 1% ?˜?Š” 10% ë¥? local averageë¥? ?˜•?„±?•˜ê¸? ?œ„?•´ ?¬ì°©í•´?•¼ ?•˜?Š”?° ?´ ê²½ìš° ? „ì²? ê¸¸ì´?˜ 63% ?˜?Š” 80%?‚˜ ?˜?Š” ê¸¸ì´ë¥? ê°? ?ž…? ¥ ë³€?ˆ˜ë§ˆë‹¤ ?‹¤ë¤„ì•¼?•œ?‹¤?Š” ê²ƒì´?‹¤. ?´?Ÿ¬?•œ ?´?›ƒ ì§‘ë‹¨?“¤??€ ?” ?´?ƒ **'local'** ?•˜ì§€ ?•Š?‹¤.

?´? ‡?‹¤ê³? $r$ ?„ ê°ì†Œ?‹œ?‚¤ê²? ?˜ë©? ?” ? ??€ ê´€ì¸¡ì¹˜?“¤ë¡? ?‰ê· ë‚´ê¸? ?•Œë¬¸ì— ? ?•©?˜ ë¶„ì‚°??€ ì»¤ì?€ê²? ?œ?‹¤. 

ê³ ì°¨?›?—?„œ?˜ sparse sampling ?˜ ?‹¤ë¥? ê²°ê³¼ë¡œëŠ” ëª¨ë“  ?‘œë³? ì§€? ?“¤?´ ?‘œë³¸ê³µê°„ì˜ ëª¨ì„œë¦?(?•œìª?)?— ê°€ê¹ë‹¤?Š” ê²ƒì´?‹¤. 

ì¦?, ?‹¤ë¥? ê´€ì¸? ì§€? ê³? ê°€ê¹ë‹¤ê¸? ë³´ë‹¤?Š” ?‘œë³? ê³µê°„?˜ ê²½ê³„?— ?” ê°€ê¹Œì›Œë²„ë¦°?‹¤?Š” ê²ƒì´?‹¤. 

> ì°¨ì›?´ ì»¤ì§ˆ ?ˆ˜ë¡? ê·? ê³µê°„ ?žì²´ëŠ” ê¸°í•˜ê¸‰ìˆ˜? ?œ¼ë¡? ì¦ê?€?•˜ë¯€ë¡? ?•œìª½ì— ëª°ë ¤?žˆê²? ?œ?‹¤?Š” ?‹?˜ ?‘œ?˜„?¸?“¯?•˜?‹¤. ?´?Š” ê³ ì°¨?›?˜ ?°?´?„°?¼ ?ˆ˜ë¡? ê°™ì?€ ê³µê°„?„ ì°¨ì?€?•˜ê¸? ?œ„?•´ ?‹¨ì°¨ì›?˜ ?ƒ˜?”Œ ê°??ˆ˜ $N^p$ ê°€ ?•„?š”?•˜?‹¤?Š” ?‘œ?˜„ê³? ?—°ê²°ëœ?‹¤. (ì°¨ì?€?•˜?Š” ê³µê°„ = $N^{1/p}$)

?´ê²? ë¬¸ì œê°€ ?˜?Š” ?´?œ ?Š” ë°”ë¡œ training ?‘œë³¸ì˜ ëª¨ì„œë¦? ê·¼ì²˜?—?„œ?Š” ?˜ˆì¸¡ì´ ?” ?–´? µê¸? ?•Œë¬¸ì´?‹¤. 

> One must extrapolate from neighboring sample points rather than interpolate between them (?)


#![](curseofdimension.PNG)

$U_p(-1,1)$ ?—?„œ 1000ê°œì˜ training samples?„ ?ƒ?„±?•˜?—¬ ?‹œë®¬ë ˆ?´?…˜?„ ì§„í–‰?•´ë³´ìž. 

$X$??€ $Y$?˜ true relationship?„ ?•„?ž˜??€ ê°™ì´ ê°€? •?•œ?‹¤.

$$Y=f(X)=e^{-8||X||^2}$$

1-Nearest Neighbor ê·œì¹™?„ ?‚¬?š©?•˜?—¬ test point $x_0=0$ ?—?„œ?˜ $y_0$ ?„ ?˜ˆì¸¡í•˜ê³ ìž ?•œ?‹¤. 

training set?„ $T$ ë¡? ?‘œê¸°í•˜?—¬ ?š°ë¦¬ëŠ” expected prediction error at $x_0$ ?„ 1000ê°œì˜ sample?— ??€?•´ ?‰ê· ë‚´?–´ ê³„ì‚°?•  ?ˆ˜ ?žˆ?‹¤. 

ë¬¸ì œê°€ deterministic ?•˜ë¯€ë¡? ?´?Š” $f(0)$ ?— ??€?•œ MSE?´?‹¤.

$$\text{MSE}(x_0)=E_{T}[(f(x_0)-\hat{y}_0)^2]=$$

$$E_T[(f(x_0)-E_{T}(\hat{y}_0)+E_{T}(\hat{y}_0)-\hat{y}_0)^2]=$$

> interaction term (2XY) is 0

$$E_{T}[(\hat{y}_0-E_{T}(\hat{y}_0))^2]+(E_{T}[\hat{y}_0]-f(x_0))^2=\text{Var}_{T}(\hat{y}_0)+\text{Bias}^2(\hat{y}_0)$$

> Bias-Variance Decomposition (Always Possible and Often Useful)

### Simulation

```{r}
## curse of dimensionality example
set.seed(2013122044)

xgenerator=function(n,p){
  # 1000 training saples x_i generated        uniformly on [-1,1]^p
  X=data.frame(matrix(runif(n*p,-1,1),nrow=n,ncol=p))
  colnames(X)=paste0('X',1:p)
  return (X)
}

# assume that the true relationship         between X and Y is without any measurement   error
true_f=function(X){
  Y=numeric(nrow(X))
  for (i in 1:nrow(X)){
    Y[i]=exp(-8*(sum(X[i,]^2)))  
  }
  return (Y)
}

# we use 1-nn rule to predict y_0 at the    test point x_0=0

euc_dist=function(xvec,p){
  test=data.frame(t(rep(0,p)))
  return (sqrt(sum((xvec-test)^2)))
}

knnsimulator=function(n,k,p){
  X=xgenerator(n,p)
  dist=apply(X,1,euc_dist,p=p)
  knn=X[order(dist)[1:k],]
  return (knn)
}
```

```{r}
# True relationship in one dimension
X=xgenerator(1000,1)
f=true_f(X)
df=cbind(X,f)[order(X),]

ggplot(df,aes(x=X1,y=f))+
  geom_line()+
  labs(x='X',y='f(X)')+
  ggtitle('True Relationship in 1 Dimension')+
  theme_bw()+
  theme(plot.title=element_text(face='bold'))

```

```{r}
# simulation of knn (p=2,k=10)
X=xgenerator(n=1000,p=2)
knn=knnsimulator(n=1000,p=2,k=10)
test=data.frame(t(rep(0,2)))
  
ggplot(X)+
  geom_point(aes(x=X1,y=X2),color='darkgrey')+
  geom_point(data=test,aes(x=X1,y=X2,color='red'))+
  geom_point(data=knn,aes(x=X1,y=X2,color='orange'))+
  scale_color_manual(name=NULL,
            values=c('red'='red','orange'='orange'),
            labels=c('Test point','Nearest neighborhood'))+
  ggtitle('10-Nearest Neighbors in Two Dimension')+
  theme_bw()+
  theme(plot.title=element_text(face='bold'))+
  coord_fixed()
```

```{r}
# simulation of distance to 1-nn vs. dimension

# (y0=1)

distance_sim=function(p,n,nsim){
  # distance
  distance=numeric(nsim)
  # yhat
  yhat=numeric(nsim)
  # test data point (x0=0)
  test=data.frame(t(rep(0,p)))
  # iteration
  for (i in 1:nsim){
    X=xgenerator(n,p)
    distance[i]=min(apply(X,1,euc_dist,p=p))
    nn=X[which.min(apply(X,1,euc_dist,p=p)),]
    yhat[i]=true_f(data.frame(nn))
  }
  # sample variance
  variance=mean((mean(yhat)-yhat)^2)
  # sample squared bias
  squared_bias=(1-mean(yhat))^2
  
  # results
  result=data.frame(Mean_Distance=mean(distance),
                    Variance=variance,
                  Squared_Bias=squared_bias,
                  MSE=variance+squared_bias)
  
  return (result)
}
```

```{r}
# 100 simulations for each dimension, 100 samples for 1~10 dimensions

result=distance_sim(p=1,n=500,nsim=100)
for (i in 2:10){
  result=rbind(result,distance_sim(p=i,n=500,nsim=100))
  
}


# visualizations

result$Dimension=1:10

# average distance to nearest neighbor
ggplot(result,aes(x=Dimension,y=Mean_Distance))+
  geom_point(color='tomato',size=3)+
  geom_line(color='tomato',size=1)+
  theme_bw()+
  ggtitle('Average Distance to Nearest Neighbor')+
  theme(plot.title=element_text(face='bold'))+
  labs(y='Average Distance')

result2=gather(result[,-1],key,value,-Dimension,factor_key=T)


# variance, squared bias, and MSE
ggplot(result2,aes(x=Dimension,y=value,color=key))+
  geom_point(size=3)+
  geom_line(size=1)+
  theme_bw()+
  ggtitle('MSE vs. Dimension')+
  theme(plot.title=element_text(face='bold'),
        legend.title=element_blank())+
  labs(y='MSE')

```

?´ë¥? ?†µ?•´ ì°¨ì›?´ ì»¤ì§ˆ?ˆ˜ë¡? ê°€?ž¥ ê°€ê¹Œìš´ ?´?›ƒê³¼ì˜ ê±°ë¦¬ ?žì²´ê?€ ì»¤ì?€ë¯€ë¡? ? ˆ??€? ?¸ Bias??€ Variance ?˜?•œ ì»¤ì?€ê²? ?˜?Š” ê²ƒì„ ?™•?¸?•  ?ˆ˜ ?žˆ?‹¤. 

**?Š¹ë³„ížˆ** Bias term?˜ ê±°ë¦¬?— ??€?•œ ì¢…ì†?„±??€ ?‹¤? œ function?— ?˜?–¥?„ ë°›ê³  ê·? function?´ ?•­?ƒ ??€ì°¨ì›?—ë§? ?˜?–¥?„ ì£¼ëŠ” ê²½ìš°?—?Š” ë¶„ì‚°?´ ì§€ë°°ì ?´ê²? ?œ?‹¤.

### What good about the linear model?

> ? ?•©?˜?Š” ëª¨ë¸?— ??€?•œ ê°€?ž¥ ? œ?•½?„ ê±¸ì–´ ì°¨ì›?˜ ??€ì£¼ì—?„œ ë²—ì–´?‚  ?ˆ˜ ?žˆ?‹¤.


$$Y=X^T\beta+\epsilon$$

?œ„??€ ê°™ì?€ ?„ ?˜• ëª¨í˜•?„ ê°€? •?•˜?ž. $(\epsilon\sim{N(0,\sigma^2))}$

?š°ë¦¬ëŠ” ëª¨ë¸?„ training data?— ??€?•œ least squares ë°©ë²•?œ¼ë¡? ? ?•©?•˜?—¬ ?ž„?˜?˜ test point $x_0$ ?— ??€?•´ $\hat{y}_0=x^T_0\hat{\beta}$ ë¥? ê°–ê³  

$$\hat{y}_0=x^T_0\beta+\sum_{i=1}^{N}l_i(x_0)\epsilon_i$$

(where $l_i(x_0)$ is the $i$th element of $\boldsymbol{X(X^TX)^{-1}}x_0$)

?´?Ÿ¬?•œ ëª¨ë¸ ?•˜?—?„œ least square estimates ?Š” unbiased ?•˜ë¯€ë¡?, ?š°ë¦¬ëŠ” ?•„?ž˜??€ ê°™ì´ EPEë¥? ?‚˜??€?‚¼ ?ˆ˜ ?žˆ?‹¤.

$$\text{EPE}(x_0)=E_{y_0|x_0}E_T[(y_0-\hat{y}_0)^2]\\=Var(y_0|x_0)+E_T(\hat{y}_0-E_T\hat{y}_0)^2+(E_T[\hat{y}_0-x_0^T\beta])^2$$

$$\therefore{\sigma^2+E_T[x_0^T(\boldsymbol{(X^TX})^{-1}x_0\sigma^2]+0^2}$$

variance?Š” $x_0$ ?— ì¢…ì†? ?´ê³?, $N$ ?´ ?¬ê³? $T$ ê°€ random?•˜ê²? ì¶”ì¶œ?˜?—ˆ?‹¤ë©?, $E(X)=0$ ?„ ê°€? •?•˜ë©? $\boldsymbol{X^TX}\to{N\text{Cov}(X)}$ ?´ê³?

$$E_{x_0}\text{EPE}(x_0)\sim{E_{x_0}[x_0^T\text{Cov}(X)^{-1}x_0\sigma^2/N+\sigma^2]}\\=\text{trace}[\text{Cov}(X)^{-1}\text{Cov}(x_0)]\sigma^2/N+\sigma^2\\=\sigma^2(p/N)+\sigma^2$$

?œ„ë¥? ?†µ?•´ expected squared predicted errorê°€ $p$ ?— ??€?•œ ?„ ?˜•ì¦ê?€?•¨?ˆ˜?ž„?„ ?•Œ ?ˆ˜ ?žˆ?‹¤. (ê¸°ìš¸ê¸? $\sigma^2/N$) 

?”°?¼?„œ $N$ ?´ ?¬ê±°ë‚˜ ?™?‹œ?— $\sigma^2$ ?´ ?ž‘?‹¤ë©?, ?´?Ÿ¬?•œ ë¶„ì‚°?˜ ì¦ê?€?Š” ë¬´ì‹œ?•  ?ˆ˜ ?žˆê²? ?  ? •?„ë¡? ?ž‘?•„ì§„ë‹¤. (deterministic?•œ ê²½ìš° 0)

**ì¦?,** ?—„ê²©í•œ ê°€? • ?‚¬?•­ ?•˜?—?„œ, ?„ ?˜• ëª¨ë¸??€ unbiased ?•˜ë©? ë¬´ì‹œ?•  ? •?„ë¡? ë¶„ì‚°?´ ?ž‘?‹¤. ?•˜ì§€ë§? ?´?Ÿ¬?•œ ê°€? •?“¤?´ ??€ë¦? ê²½ìš°?—?Š” 1-NN?´ ?›”?“±?•  ê²ƒì´?‹¤. 

?›„?ˆ ?•  ?—¬?Ÿ¬ ëª¨ë¸?“¤??€ ?´?Ÿ¬?•œ ?—„ê²©í•œ ?„ ?˜• ëª¨ë¸ê³? ê·¹ë„ë¡? ?œ ?—°?•œ 1NN ?‚¬?´ë¥? ?˜¤ê°€ë©? ê³ ì°¨?›?—?„œ?˜ ?•¨?ˆ˜?˜ ë³µìž¡?„?˜ ì§€?ˆ˜?  ì¦ê?€ë¥? ?”¼?•˜ê¸? ?œ„?•´ ? œ?•ˆ?œ ê°ìž?˜ ê°€? •ê³? ?Ž¸?–¥?“¤?— ??€?•´ ë°°ìš¸ ê²ƒì´?‹¤.

## Statistical Models, Supervised Learning and Function Approximation

*"finding a useful approximation of $f(x)$"*

?´ë¡ ì  ë°°ê²½ ?•˜?—?„œ ?š°ë¦¬ëŠ” squared error loss ê°€ ?šŒê·€ ?•¨?ˆ˜ $f(x)=E(Y|X=x)$ ë¥? ?„ì¶œí•œ?‹¤ê³? ë°°ì› ?‹¤. 

ìµœê·¼? ‘ ?´?›ƒ ë°©ë²•?˜ ê²½ìš° ?´?Ÿ¬?•œ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì— ??€?•œ **ì§ì ‘? ?¸ ì¶”ì •ì¹?**?“¤ë¡? ë³? ?ˆ˜ ?žˆê³?, ìµœì†Œ?•œ ?•„?ž˜ ?‘ ê°œì˜ ê²½ìš°?—?„œ ?˜¤?ž‘?™?•˜?Š” ê²ƒì„ ?™•?¸?•˜??€?‹¤.

* ?ž…? ¥ ê³µê°„?˜ ì°¨ì›?´ ?†’?‹¤ë©? ìµœê·¼? ‘ ?´?›ƒ?“¤?´ ëª©í‘œ ì§€? ?—?„œ ë©€?–´ì§€ê²? ?˜ê³?, ?° ?˜¤ì°¨ë?? ?‚³?Š”?‹¤.

* ?Š¹ë³„í•œ êµ¬ì¡°ê°€ ì¡´ìž¬?•œ?‹¤ê³? ?•˜ë©? ?´ë¥? ì¶”ì •ì¹˜ì˜ ?Ž¸?–¥ê³? ë¶„ì‚°?„ ëª¨ë‘ ì¤„ì´?Š”?° ?‚¬?š©?•  ?ˆ˜ ?žˆ?‹¤.

### A Statistical Model for the Joint Distribution Pr(X,Y)

$$Y=f(X)+\epsilon$$

?´ ?žˆ?‹¤ê³? ?•˜?ž.$(E(\epsilon)=0$ and independent of $X)$

?´?Ÿ¬?•œ ëª¨ë¸ $f(x)=E(Y|X=x)$ ?— ??€?•´, ?‹¤? œë¡? ì¡°ê±´ë¶€ ë¶„í¬ $Pr(Y|X)$ ?Š” $X$ ?— ?˜¤ì§? ì¡°ê±´ë¶€ ?‰ê·? $f(x)$ ë¥? ?†µ?•´ ?˜ì¡´í•œ?‹¤.

?´?Ÿ¬?•œ ê°€ë²? ?˜¤ì°? (additive error) ëª¨ë¸??€ ?‹¤? œ ê´€ê³„ì— ??€?•œ ?œ ?š©?•œ ê·¼ì‚¬ ë°©ë²•?´?‹¤. 

?” ?‚˜?•„ê°€ ?—¬?Ÿ¬ ì²´ê³„?—?„œ ?ž…ì¶œë ¥?Œ $(X,Y)$ ?Š” deterministic relationship $Y=f(x)$ ë¥? ê°–ì?€ ?•Š?„ ê²ƒì´?‹¤. 

?¼ë°˜ì ?œ¼ë¡? **?‹¤ë¥? ì¸¡ì •?˜ì§€ ?•Š??€ ë³€?ˆ˜?“¤**?´ $Y$ ?— ?˜?–¥?„ ë¯¸ì¹  ê²ƒì´ë©?, ?´ ?˜?•œ ì¸¡ì • ?˜¤ì°¨ë?? ?ˆ˜ë°˜í•  ê²ƒì´?‹¤. 

ê°€ë²? ëª¨ë¸??€ deterministic ?•œ ê´€ê³„ì—?„œ ë²—ì–´?‚œ ?´?Ÿ¬?•œ ë¶€ë¶„ë“¤?„ ?˜¤ì°¨í•­ $\epsilon$ ?„ ?†µ?•´ ?ž¡?•„?‚¼ ?ˆ˜ ?žˆ?‹¤ê³? ê°€? •?•œ?‹¤.

?Š¹? • ë¬¸ì œ?“¤?˜ ê²½ìš° deterministic relationship?´ ?„±ë¦½í•œ?‹¤. ë§Žì?€ ë¶„ë¥˜ ë¬¸ì œ?“¤?´ ?´?Ÿ¬?•œ ?˜•?ƒœ?´ë©?, response surfaceê°€ $R^p$ ì°¨ì›?—?„œ ? •?˜?œ ?ƒ‰ì¹ ëœ ì§€?„?˜ ?˜•?ƒœë¡? ?ƒê°ë  ?ˆ˜ ?žˆ?‹¤.

training data?Š” ?´?Ÿ¬?•œ ì§€?„ ${x_i,g_i}$ ?—?„œ ?ƒ‰ì¹ ëœ ?˜ˆ?‹œ?“¤ë¡? ?´ë£¨ì–´? ¸ ?žˆê³?, ?š°ë¦¬ì˜ ëª©í‘œ?Š” ?‹¤ë¥? ?–´?–¤ ì§€?  ?˜?•œ ?ƒ‰ì¹ í•  ?ˆ˜ ?žˆ?Š” ê²ƒì´?‹¤.

?´ ê²½ìš° ?•¨?ˆ˜?Š” ê²°ì •ë¡ ì ?´ê³?, randomness?Š” training points?“¤?˜ $x$ ?œ„ì¹˜ë?? ?†µ?•´ ë°œìƒ?•œ?‹¤.

> Why is it deterministic?

> ?˜¤ì°¨í•­?— ê¸°ë°˜?•œ ëª¨ë¸?“¤?— ? ?•©?•œ ê¸°ìˆ ?“¤ë¡? ?´?Ÿ¬?•œ ë¬¸ì œ?“¤?´ ?•´ê²°ë  ?ˆ˜ ?žˆ?Š”ê°€?

?œ„?—?„œ ?„œ?ˆ ?•œ ê´€ê³„ì‹?˜ ê°€? •(?˜¤ì°¨í•­?˜ ?…ë¦½ì„±, ?“±ë¶„ì‚°?„±, ? •ê·œì„±) ??€ ë°˜ë“œ?‹œ ?•„?š”?•˜ì§€?Š” ?•Šì§€ë§?, ?š°ë¦¬ê?€ ?˜¤ì°? ? œê³±ì„ ?‚°?ˆ ?‰ê· ë‚¼ ?•Œë¥? ?ƒê°í•˜ë©? ?•©?‹¹?•˜?‹¤. (in $EPE$ Criterion)

?´?Ÿ¬?•œ ëª¨ë¸?˜ ê²½ìš° least squaresë¥? ëª¨ë¸ ì¶”ì •?— ?žˆ?–´?„œ?˜ ê¸°ì?€?œ¼ë¡? ?™œ?š©?•˜?Š” ê²ƒì´ ?•©?‹¹?•˜?‹¤. 

**?˜¤ì°¨ì˜ ?…ë¦½ì„± ê°€? •**?„ ?”¼?•˜ê¸? ?œ„?•œ ?‹¨?ˆœ?•œ ë³€?˜•?˜ ë°©ë²•?„ ?žˆ?‹¤. ?˜ˆë¥? ?“¤?–´, ?š°ë¦¬ëŠ” $Var(Y|X=x)=\sigma(x)$ ë¥? ë§Œë“¤?–´ ?‰ê· ê³¼ ë¶„ì‚°?´ ëª¨ë‘ $X$?— ?˜ì¡´í•˜ê²? ?•  ?ˆ˜ ?žˆ?‹¤.

?¼ë°˜ì ?œ¼ë¡? ì¡°ê±´ë¶€ ë¶„í¬ $Pr(Y|X)$ ?Š” $X$?— ë³µìž¡?•œ ë°©ë²•?œ¼ë¡? ì¢…ì†? ?¼ ?ˆ˜ ?žˆì§€ë§? ê°€ë²? ?˜¤ì°? ëª¨ë¸??€ ?´ë¥? **ë°°ì œ?•œ?‹¤.**

ê°€ë²? ?˜¤ì°? ëª¨ë¸??€ ?¼ë°˜ì ?œ¼ë¡? ì§ˆì  ì¢…ì† ë³€?ˆ˜ $G$ ?—?Š” ?‚¬?š©?˜ì§€ ?•Š?Š”?‹¤. ?´ ê²½ìš° ëª©í‘œ ?•¨?ˆ˜ $p(X)$ ?Š” ì¡°ê±´ë¶€ ë°€?„ $Pr(G|X)$ ?´ê³?, ?´?Š” ì§ì ‘? ?œ¼ë¡? ëª¨ë¸ë§ë  ?ˆ˜ ?žˆ?‹¤.

?˜ˆë¥? ?“¤?–´, ?‘ ê°œì˜ ë²”ì£¼ë¥? ê°–ëŠ” ?žë£Œì˜ ê²½ìš°, ?žë£Œê?€ ?…ë¦½ì ?¸ ?´ì§? ?‹œ?–‰?—?„œ ?ƒ?„±?˜?—ˆ?‹¤ê³? ê°€? •?•˜?Š” ê²ƒì´ ?•©ë¦¬ì ?´?‹¤. (ê·? ì¤? ?•œ ê°? ë²”ì£¼?˜ ?žë£? ì¶”ì¶œ ?™•ë¥ ì´ $p(X)$ë¥? ê°–ëŠ”)

?”°?¼?„œ $Y$ ê°€ 0-1ë¡? ì½”ë”©?˜?—ˆ?‹¤ë©? $E(Y|X=x)=p(x)$ ê°€ ?˜ì§€ë§? ë¶„ì‚°?´ $x$?— **ì¢…ì†? **?´?‹¤. $(Var(Y|X=x)=p(x)\times(1-p(x)))$

### Supervised Learning

ë¨¸ì‹  ?Ÿ¬?‹ ê´€? ?—?„œ?˜ ?•¨?ˆ˜ ? ?•© ?Œ¨?Ÿ¬?‹¤?ž„??€ training setê³? ?ž…ì¶œë ¥?´ ì¡´ìž¬?•˜?Š” ?‹œ?Š¤?…œ?—?„œ 
?•™?Šµ ?•Œê³ ë¦¬ì¦˜ì„ ?†µ?•´ ê·¼ì‚¬ê°€ ?´ë£¨ì–´ì§„ë‹¤.

?´?Ÿ¬?•œ ?•™?Šµ ?•Œê³ ë¦¬ì¦˜ì?€ ?ž…ì¶œë ¥ ê´€ê³? $\hat{f}$ ë¥? ?› ?žë£Œì?€ ?ƒ?„±?œ ì¶œë ¥ê°’ì˜ ì°¨ì´ $y_i-\hat{f}(x_i)$ ë¥? ?†µ?•´ ë³€?˜•?•  ?ˆ˜ ?žˆ?‹¤?Š” ?„±ì§ˆì„ ê°–ê³  ?´?Ÿ¬?•œ ê³¼ì •?„ **learning by example** ?´?¼ê³? ?•œ?‹¤. 

### Function Approximation

?†µê³?, ?ˆ˜?•™?  ê´€? ?—?„œ?˜ ?´?Ÿ¬?•œ ?•™?Šµ ?Œ¨?Ÿ¬?‹¤?ž„??€ **?•¨?ˆ˜ ê·¼ì‚¬** ?´ë©? ë¨¸ì‹  ?Ÿ¬?‹ ë¶„ì•¼ (?¸ê°„ì˜ ì¶”ë¦¬ ê³¼ì •?„ ëª¨ë°©) ??€ ?‹ ê²½ë§ (?‡Œ?˜ ?ƒë¬¼í•™?  ?„±ì§? ëª¨ë°©) ?— ?‚¬?š©?˜?—ˆ?‹¤. 

ë§Žì?€ ê·¼ì‚¬ ë°©ë²•?“¤??€ ?žë£Œì— ? ?•©?•˜ê²? ë³€?˜•?  ?ˆ˜ ?žˆ?Š” ëª¨ìˆ˜ ì§‘í•© $\theta$ ë¥? ê°–ëŠ”?‹¤. 

?˜ˆë¥? ?“¤?–´, ?„ ?˜• ëª¨ë¸ $f(x)=x^T\beta$ ?Š” $\theta$ ë¥? $\beta$ ë¡? ê°–ëŠ”?‹¤. 

?˜ ?‹¤ë¥? ?œ ?š©?•œ ê·¼ì‚¬ê¸°ëŠ” **linear basis expansion** ?œ¼ë¡? ?‘œ?˜„?  ?ˆ˜ ?žˆ?‹¤. 

$$f_\theta(x)=\sum_{k=1}^{K}h_k(x)\theta_k$$

$h_k$ ?Š” ? ?•©?•œ ?•¨?ˆ˜?˜ ì§‘í•© ?˜¹??€ ?ž…? ¥ ë²¡í„° $x$ ?˜ ë³€?™˜?´?‹¤. ? „?†µ?  ?˜ˆ?‹œë¡? polynomial & trigonometric expansion?´ ?žˆê³? $h_k$ ê°€ $x_1^2, x_1x_2^2, \text{cos}(x_1)$ ?“±ê³? ê°™ì?€ ê²½ìš°ë¥? ë§í•œ?‹¤.

?š°ë¦¬ëŠ” ë¹„ì„ ?˜• ?™•?ž¥ ?˜?•œ ë§ˆì£¼?•˜ê²? ? ?…?°, ?‹ ê²½ë§ ëª¨ë¸?—?„œ ?‚¬?š©?˜?Š” sigmoid ë³€?™˜?´ ??€?‘œ?  ?˜ˆ?´?‹¤.

$$h_k(x)=\frac{1}{1+\text{exp}(-x^T\beta_k)}$$

?š°ë¦¬ëŠ” least squareë¡? residual sum of squaresë¥? ëª¨ìˆ˜?˜ ?•¨?ˆ˜ë¡? ì·¨ê¸‰, ìµœì†Œ?™”?‹œì¼? ëª¨ìˆ˜?“¤ $\theta$ ë¥? ì¶”ì •?•˜?Š” ?°?— ?‚¬?š©?•  ?ˆ˜ ?žˆ?—ˆ?‹¤.

$$\text{RSS}(\theta)=\sum_{i=1}^{N}(y_i-f_{\theta}(x_i))^2$$

?„ ?˜• ëª¨ë¸?˜ ê²½ìš° ?‹¨?ˆœ?•œ closed-form ?˜ ?•´ë¥? ìµœì†Œ?™” ë¬¸ì œ?—?„œ ?–»?„ ?ˆ˜ ?žˆ?‹¤. ?´?Š” basis function ë°©ë²•ë¡ ì—?„œ?„ ?‹¤ë¥? ?–´?–¤ ?ž ?ž¬ ëª¨ìˆ˜?“¤?´ ì¡´ìž¬?•˜ì§€ ?•Š?Š”?•œ ?™?¼?•˜?‹¤. 

ë§Œì•½ ?‹¤ë¥? ê²½ìš°?—?Š” ?•´ê°€ iterative methods ?˜¹??€ numerical optimization?„ ?†µ?•´ ?–»?–´? ¸?•¼ ?•œ?‹¤.

least squares ë§ê³ ?„ ì¶”ì •?—?„œ ?„ë¦? ?‚¬?š©?˜?Š” ê°œë…?´ ë°”ë¡œ **maximum likelihood estimation** ?´?‹¤.

$$L(\theta)=\prod_{i=1}^{n}f(x_i;\theta)$$

$$l(\theta)=\sum_{i=1}^{n}\text{log}f(x_i;\theta)$$

MLEë¥? ?–»?Š” ê³¼ì •?˜ ?›ë¦¬ëŠ” ?‹¨?ˆœ?•˜?‹¤. $\theta$ ?˜ ê°’ìœ¼ë¡? ê°€?ž¥ ?•©ë¦¬ì ?¸ ê²½ìš°?Š” **ê´€ì¸¡ëœ ?‘œë³¸ì„ ?–»?„ ?™•ë¥ ì´ ê°€?ž¥ ?†’??€ ê²½ìš°**?´?‹¤.

?š°ë¦¬ëŠ” ë°°ì› ?“¯?´ ê°€ë²? ?˜¤ì°? ëª¨ë¸ $Y=f_\theta(X)+\epsilon$ $(\epsilon\sim{\text{N}(0,\sigma^2)})$?— ??€?•œ least squares estimator ê°€ ì¡°ê±´ë¶€ ê°€?Š¥?„ë¥? ?‚¬?š©?•œ maximum likelihood estimator??€ ê°™ë‹¤?Š” ê²ƒì„ ?•Œê³? ?žˆ?‹¤.

$$Pr(Y|X,\theta)=N(f_\theta(X),\sigma^2)$$

?”°?¼?„œ ì¶”ê?€? ?¸ ? •ê·œì„± ê°€? •?´ ë³´ë‹¤ ? œ?•œ? ?¼ ê²? ê°™ì•„ ë³´ì—¬?„, ê²°ê³¼?Š” ê°™ë‹¤.

?žë£Œì˜ log-likelihood?Š” ?•„?ž˜??€ ê°™ê³ 

$$l(\theta)=-\frac{N}{2}\text{log}(2\pi)-N\text{log}\sigma-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i-f_\theta(x_i))^2$$

$\theta$ ë¥? ?¬?•¨?•˜?Š” ?•­??€ ?˜¤ì§? ë§ˆì?€ë§? ?•­?´ë©?, ?´?Š” $\text{RSS}(\theta)$ ?— ??€?•œ scalar negative multiplier ?´?‹¤. 

ë³´ë‹¤ ?¥ë¯¸ë¡œ?š´ ?˜ˆ?‹œ?Š” ì§ˆì  ì¢…ì† ë³€?ˆ˜ $G$?— ??€?•œ ?šŒê·€ ?•¨?ˆ˜ $Pr(G|X)$ ë¥? ?œ„?•œ ?‹¤?•­ ë¶„í¬ likelihood ê²½ìš°?´?‹¤.

?š°ë¦¬ê?€ ì£¼ì–´ì§? $X$ ?— ??€?•œ ê°? ë²”ì£¼?— ?†?•  ì¡°ê±´ë¶€ ?™•ë¥ ì„ ?œ„?•œ ëª¨ë¸ $Pr(G=G_k|X=x)=p_{k,\theta}(x),\text{ }k=1,...,K$ ê°€ ?žˆ?‹¤ê³? ?•˜?ž.

?´ ê²½ìš°?˜ log-likelihood?Š” **(cross-entropy)** ?•„?ž˜??€ ê°™ë‹¤.

$$l(\theta)=\sum_{i=1}^{N}\text{log}p_{g_i,\theta}(x_i)$$

## Structured Regression Models

?ž„?˜?˜ ?•¨?ˆ˜ $f$ ?— ??€?•œ RSS ê¸°ì?€?„ ê³ ë ¤?•´ë³´ìž. 

$$\text{RSS}(f)=\sum_{i=1}^{N}(y_i-f(x_i))^2$$

?œ„?˜ ?‹?„ ìµœì†Œ?™”?•˜?Š” ê²ƒì?€ ë¬´ìˆ˜?•œ ë§Žì?€ ?•´?“¤?„ ?‚³?Š”?‹¤ : training points $(x_i,y_i)$ ë¥? ì§€?‚˜?Š” ê·? ?–´?–¤ ?•¨?ˆ˜ $\hat{f}$ ê°€ ëª¨ë‘ ?•´ê°€ ?  ?ˆ˜ ?žˆ?‹¤. 

?”°?¼?„œ ?–´?–¤ ?Š¹? • ?•´?Š” test points?— ??€?•´ ?˜•?Ž¸?—†?Š” ?˜ˆì¸? ?„±?Š¥?„ ë³´ì—¬ì¤? ê²ƒì´?‹¤. 

ë§Œì¼ ê´€ì¸¡ì¹˜ ?Œ?˜ ê°??ˆ˜ê°€ ë§Žë‹¤ë©? ?´?Ÿ¬?•œ ?œ„?—˜??€ ? œ?•œ?  ê²ƒì´?‹¤. 

?œ ?•œ?•œ $N$ ?—?„œ ?œ ?š©?•œ ê²°ê³¼ë¥? ?–»ê¸? ?œ„?•´?„œ ?š°ë¦¬ëŠ” ?œ„?˜ ìµœì†Œ?™” ë¬¸ì œ?— ? œ?•½?„ ê±¸ì–´ ë³´ë‹¤ ?ž‘??€ ?•¨?ˆ˜?˜ ì§‘í•©?„ ?–»?–´?•¼?•œ?‹¤. 

?´?Ÿ¬?•œ ? œ?•½?˜ ê²°ì •?„ ?œ„?•œ ê³¼ì •??€ ?žë£Œì˜ ?™¸ë¶€?—?„œ ?´ë£¨ì–´ì§„ë‹¤. ?´?Ÿ¬?•œ ? œ?•½?“¤??€ ?•Œ?•Œë¡? $f_\theta$ ?˜ ëª¨ìˆ˜?  ?ž¬?‘œ?˜„?œ¼ë¡? ?´ë£¨ì–´ì§€ê±°ë‚˜ ?•™?Šµ ë°©ë²• ?žì²´ì—?„œ ?„¤ê³„ëœ?‹¤. 

?´?Ÿ¬?•œ ?•´?˜ ? œ?•½?“¤??€ ?´ ì±…ì˜ **ì£¼ìš” ì£¼ì œ**?´?‹¤. 

ëª…ì‹¬?•´?•¼?•  ê²ƒì?€, ê·? ?–´?– ?•œ $f$ ?— ì£¼ì–´ì§€?Š” ? œ?•½?“¤?´ ?‹¤? œë¡? ?•´?˜ ë³µìž¡?„±?œ¼ë¡? ?¸?•´ ë°œìƒ?•˜?Š” ëª¨í˜¸?•¨?„ ? œê±°í•  ?ˆ˜?Š” ?—†?‹¤?Š” ê²ƒì´?‹¤.

ê°€?Š¥?•œ ë§Žì?€ ? œ?•½?“¤?´ ì¡´ìž¬?•˜ì§€ë§?, ê°ê°??€ ?œ ?¼ ?•´ë¡? ?´?–´ì§€ë©?, ?”°?¼?„œ ?• ë§¤í•¨??€ ?–´?–¤ ? œ?•½?„ ?„ ?ƒ?•  ê²ƒì´?ƒ?˜ ë¬¸ì œë¡? ? „?™˜?  ?ˆ˜ ?žˆ?‹¤.

?¼ë°˜ì ?œ¼ë¡? ??€ë¶€ë¶„ì˜ ?•™?Šµ ë°©ë²•?—?„œ ? ?š©?•˜?Š” ? œ?•½?“¤??€ ? œ?•½?˜ **ë³µìž¡?„** ë¡? ê¸°ìˆ ?  ?ˆ˜ ?žˆ?‹¤.

> ì¶”ê?€?  ?‚´?š© ì¡´ìž¬.. ?–´? ¤??€

## Classes of Restricted Estimators

ë¹„ëª¨?ˆ˜?  ?šŒê·€ ê¸°ë²• ?˜?Š” ?—¬?Ÿ¬ ì¢…ë¥˜ë¡? ?‚˜?‰˜?Š” ?•™?Šµ ë°©ë²•?˜ ?‹¤?–‘?„±?“¤??€ ? ?š©?˜?Š” ? œ?•½?˜ nature?— ?˜?•´ ê²°ì •?œ?‹¤. 

?´?Ÿ¬?•œ ?—¬?Ÿ¬ ì¢…ë¥˜?˜ ? œ?•½?“¤??€ ì¢…ì¢… **smoothing** ëª¨ìˆ˜?¼ê³? ë¶ˆë¦¬?Š” ?•œ ê°? ?´?ƒ?˜ ëª¨ìˆ˜?“¤ê³? ê´€? ¨?žˆ?‹¤.

?—¬ê¸°ì„œ?Š” ?„¸ ê°€ì§€?˜ ?„“??€ ?˜ë¯¸ì˜ ë¶„ë¥˜ë¥? ?†Œê°œí•˜ê³? ?žˆ?‹¤.

### 1. Roughness Penalty and Bayesian Methods

$$\text{PRSS}(f;\lambda)=\text{RSS}(f)+\lambda{J}(f)$$

?˜ˆë¥? ?“¤?–´, 1ì°¨ì› ?ž…? ¥ ë²¡í„°?— ??€?•´ ?œ ëª…í•œ **cubic smoothing spline** ë°©ë²•??€ ?•„?ž˜?˜ penalized least-squares criterion?˜ ?•´?´?‹¤.

$$\text{PRSS}(f;\lambda)=\sum_{i=1}^{N}(y_i-f(x_i))^2+\lambda\int[f^{\prime\prime}(x)]^2dx$$

?´ ê²½ìš°?˜ ?Ž˜?„?‹°?•­?˜ roughness ?Š” $f$ ?˜ 2ì°? ë¯¸ë¶„?˜ ?° ê°’ë“¤?— ?˜?–¥?„ ë§Žì´ ë¯¸ì¹˜ë©?, ?Ž˜?„?‹°?˜ ?–‘??€ $\lambda\ge0$ ?— ?˜?•´ ê²°ì •?œ?‹¤. 

ë§Œì¼ $\lambda=0$ ?¼ ê²½ìš° ?Ž˜?„?‹°ê°€ ì¡´ìž¬?•˜ì§€ ?•Šê³?, $\lambda=\infty$ ?¼ ê²½ìš° $x$ ?‚´ ?„ ?˜• ?•¨?ˆ˜ë§Œì´ ?—ˆ?š©?  ê²ƒì´?‹¤.

?Ž˜?„?‹° ?•¨?ˆ˜ $J$ ?Š” ?–´?– ?•œ ì°¨ì›?—?„œ ì¡´ìž¬?•˜?Š” ëª¨ë“  ?•¨?ˆ˜?— ?˜?•´ ?„¤ê³„ë  ?ˆ˜ ?žˆ?œ¼ë©? ?Š¹ë³„í•œ ë²„ì „?´ ?Š¹ë³„í•œ êµ¬ì¡°ë¥? ? ?š©?•˜ê¸? ?œ„?•´ ?ƒ?„±?  ?ˆ˜ ?žˆ?‹¤.

?´? ‡ê²? ?Ž˜?„?‹° ?•¨?ˆ˜, ?˜?Š” **regularization** ë°©ë²•?“¤??€ ?š°ë¦¬ì˜ ?´?Ÿ¬?•œ ì¢…ë¥˜?˜ ?•¨?ˆ˜?˜ smooth behavior?— ??€?•œ ?‚¬? „?  ë¯¿ìŒ?„ ë°˜ì˜?•˜ë©?, ?´?Š” ?‹¤? œë¡? **Bayesian Framework** ?‚´?—?„œ ? ?š© ê°€?Š¥?•˜?‹¤.

?Ž˜?„?‹° $J$ ?Š” **log-prior** ??€ ?ƒ?‘?•˜ë©?, $\text{PRSS}(f;\lambda)$ ?Š” **log-posterior distribution** ?¼ê³? ?•  ?ˆ˜ ?žˆ?œ¼ë©? $\text{PRSS}(f;\lambda)$ ë¥? ìµœì†Œ?™”?•˜?Š” ê²ƒì?€ **posterior-mode** ë¥? ì°¾ëŠ” ê²ƒê³¼ ê°™ë‹¤. 


### 2. Kernel Methods and Local Regression

?´?Ÿ¬?•œ ë°©ë²•ë¡ ë“¤??€ local neighborhood?˜ natureë¥? ? •?˜?•¨?„ ?†µ?•´ ?šŒê·€ ?•¨?ˆ˜?˜ ì¶”ì •ì¹? ?˜?Š” ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì„ ëª…ì‹œ? ?œ¼ë¡? ? œê³µí•œ?‹¤.

local neighborhood?Š” **kernel function** $K_\lambda(x_0,x)$ ë¥? ?†µ?•´ ? •?˜?˜ë©? ?´?Š” $x_0$ ì£¼ë?€?˜ $x$ ?˜ ì§€? ?“¤?— ê°€ì¤‘ì¹˜ë¥? ë¶€?—¬?•œ?‹¤.

?˜ˆë¥? ?“¤?–´, **Gaussian Kernel** ??€ ? •ê·œí™•ë¥ ë?€?„?•¨?ˆ˜?— ê¸°ì´ˆ?•œ ê°€ì¤‘ì¹˜ ?•¨?ˆ˜ë¥? ê°–ê³ 

$$K_\lambda(x_0,x)=\frac{1}{\lambda}\text{exp}(-\frac{||x-x_0||^2}{2\lambda})$$

$x_0$ ê³¼ì˜ **squared euclidean distance** ë¥? ?†µ?•´ ë©€?–´ì§€?Š” ì§€? ?“¤?— ??€?•´?„œ ê±°ë¦¬?— ì§€?ˆ˜? ?œ¼ë¡? ê°€ì¤‘ì¹˜ë¥? ë¶€?—¬?•˜?Š” ê²ƒì´?‹¤. 

ëª¨ìˆ˜ $\lambda$ ?Š” ? •ê·œí™•ë¥ ë?€?„?˜ ë¶„ì‚°?„ ?‚˜??€?‚´ë©?, ?´?›ƒ?˜ ?„ˆë¹„ë?? ê²°ì •?•œ?‹¤.

ì»¤ë„ ì¶”ì •ì¹˜ì˜ ê°€?ž¥ ?‹¨?ˆœ?•œ ?˜•?ƒœ?Š” **Nadaraya-Watson weighted average** ?´?‹¤.

$$\hat{f}(x_0)=\frac{\sum_{i=1}^{N}K_\lambda(x_0,x_i)y_i}{\sum_{i=1}^{N}K_\lambda(x_0,x_i)}$$

?¼ë°˜ì ?œ¼ë¡? ?š°ë¦¬ëŠ” $f(x_0)$ ?— ??€?•œ local regression ì¶”ì •ì¹˜ë?? $f_\hat{\theta}(x_0)$ ë¡? ?‚˜??€?‚´ë©? 

$$\text{RSS}(f_\theta,x_0)=\sum_{i=1}^{N}K_\lambda(x_0,x_i)(y_i-f_\theta(x_i))^2$$

ë¥? ìµœì†Œ?™”?•˜?Š” $\hat{\theta}$ ?„ ì°¾ê³  $f_\theta$ ?Š” ?–´?–¤ ëª¨ìˆ˜?  ?•¨?ˆ˜?´?‹¤. (ex. ??€ì°¨ìˆ˜ ?‹¤?•­?‹)

* $f_\theta(x)=\theta_0$ , ?ƒ?ˆ˜?•¨?ˆ˜ : Nadaraya-Watson ì¶”ì •ì¹˜ì?€ ê°™ì?€ ê²°ê³¼ë¥? ?„ì¶?.

* $f_\theta(x)=\theta_0+\theta_{1}x$ , ?„ë¦? ?•Œ? ¤ì§? local linear regression model ?„ ?„ì¶?.

?”°?¼?„œ **ìµœê·¼? ‘ ?´?›ƒ ë°©ë²•**?“¤??€ ë³´ë‹¤ ?žë£Œì— ì¢…ì†? ?¸ metric?„ ê°–ëŠ” **ì»¤ë„ ë°©ë²•**?œ¼ë¡? ?ƒê°í•  ?ˆ˜ ?žˆ?‹¤.

?‹¤? œë¡?, $k$-ìµœê·¼? ‘ ?´?›ƒ?—?„œ?˜ metric??€ 

$$K_k(x,x_0)=I(||x-x_0||\le||x_{(k)}-x_0||)$$

?´ë©? $x_(k)$ ?Š” $x_0$ ??€ $k$ë²ˆì§¸ë¡? ë©€ë¦? ?žˆ?Š” ê´€ì¸¡ì¹˜ë¥? ?˜ë¯¸í•˜ê³? $I(S)$ ?Š” ì§‘í•© $S$ ?˜ indicator function?´?‹¤.

?´?Ÿ¬?•œ ë°©ë²•?“¤??€ ?‹¹?—°?žˆ ê³ ì°¨?›?—?„œ ë³€?˜•?˜?–´?•¼ ?•œ?‹¤. (ì°¨ì›?˜ ??€ì£?) 

### 3. Basis Functions and Dictonary Methods

?´ ë°©ë²•ë¡ ë“¤??€ ?µ?ˆ™?•œ ?„ ?˜• ?˜?Š” ?‹¤?•­ expansion?„ ?¬?•¨?•œ?‹¤. ?•˜ì§€ë§? ë³´ë‹¤ ì¤‘ìš”?•˜ê²? ë³µìž¡?•˜ê³? ?‹¤?–‘?•œ ëª¨ë¸?“¤?„ ?¬?•¨?•œ?‹¤. 

$f$ ?— ??€?•œ ëª¨ë¸??€ basis function?˜ linear expansion?´ë©?

$$f_\theta(x)=\sum_{m=1}^{M}\theta_mh_m(x)$$

$h_m$ ??€ ?ž…? ¥ ë²¡í„°?˜ ?•¨?ˆ˜, linear term??€ ëª¨ìˆ˜ $\theta$ ?˜ ?˜•?ƒœ?— ?˜?•´ ê²°ì •?œ?‹¤.

?´?Ÿ¬?•œ ë°©ë²•ë¡ ì?€ ë§Žì?€ ?‹¤?–‘?•œ ë°©ë²•ë¡ ë“¤?„ ?‹¤ë£? ?ˆ˜ ?žˆ?‹¤. ?–´?–¤ ê²½ìš° basis function?“¤?˜ sequenceê°€ ë¯¸ë¦¬ ê¸°ìˆ ?œ?‹¤. (basis for polynomials in $x$ of total degree $M$)

> neural net context?—?„œ ?‹¤ë¤„ì?€?Š” ?“¯? 

## Model Selection and the Bias-Variane Tradeoff

ëª¨ë“  ëª¨ë¸?“¤??€ **smoothing** ?˜?Š” **complexity** ëª¨ìˆ˜ê°€ ê²°ì •?˜?–´?•¼ ?•œ?‹¤.

* penalty term?˜ ?Š¹?ˆ˜

* kernel?˜ ?„ˆë¹?

* basis function?˜ ê°??ˆ˜

> Underfit = Large Bias, Small Variance, Overfit = Small Bias, Large Variance


